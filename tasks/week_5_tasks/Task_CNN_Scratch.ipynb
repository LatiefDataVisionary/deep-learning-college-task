{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyMOuO19+a2YhcYnv9XKQFwI",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/LatiefDataVisionary/deep-learning-college-task/blob/main/tasks/week_5_tasks/Task_CNN_Scratch.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5626ae91"
      },
      "source": [
        "# **Section 0: Initial Setup (Pengaturan Awal)**\n",
        "\n",
        "Bagian ini untuk melakukan instalasi library penting yang mungkin belum ada di Colab dan menghubungkan Google Drive."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "430e8d6e"
      },
      "source": [
        "## **0.1. Install Libraries (Instalasi Library)**\n",
        "\n",
        "Menginstal library `mtcnn` yang akan digunakan untuk deteksi wajah."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bf25ad7a",
        "outputId": "0c5e9805-c9d1-4498-dfe0-50e7b80ac314"
      },
      "source": [
        "!pip install opencv-python\n",
        "!pip install mtcnn"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: opencv-python in /usr/local/lib/python3.12/dist-packages (4.12.0.88)\n",
            "Requirement already satisfied: numpy<2.3.0,>=2 in /usr/local/lib/python3.12/dist-packages (from opencv-python) (2.0.2)\n",
            "Collecting mtcnn\n",
            "  Downloading mtcnn-1.0.0-py3-none-any.whl.metadata (5.8 kB)\n",
            "Requirement already satisfied: joblib>=1.4.2 in /usr/local/lib/python3.12/dist-packages (from mtcnn) (1.5.2)\n",
            "Collecting lz4>=4.3.3 (from mtcnn)\n",
            "  Downloading lz4-4.4.4-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (3.8 kB)\n",
            "Downloading mtcnn-1.0.0-py3-none-any.whl (1.9 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.9/1.9 MB\u001b[0m \u001b[31m66.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading lz4-4.4.4-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.3 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.3/1.3 MB\u001b[0m \u001b[31m75.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: lz4, mtcnn\n",
            "Successfully installed lz4-4.4.4 mtcnn-1.0.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8ca0d611"
      },
      "source": [
        "## **0.2. Mount Google Drive (Menghubungkan Google Drive)**\n",
        "\n",
        "Menghubungkan notebook dengan Google Drive agar dapat mengakses dataset."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bce83f40",
        "outputId": "52356c5a-f439-4923-8b9d-2ddf3d2b54c5"
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4c13c338"
      },
      "source": [
        "## **Section 1: Import Libraries and Environment Setup (Impor Library dan Pengaturan Lingkungan)**\n",
        "\n",
        "**Penjelasan:** Di sini kita akan mengimpor semua modul dan library yang dibutuhkan untuk keseluruhan proyek serta mendefinisikan variabel-variabel global seperti path direktori, ukuran gambar, dan parameter training."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9576c1c5"
      },
      "source": [
        "### **1.1. Import Core Libraries (Impor Library Utama)**\n",
        "\n",
        "**Penjelasan:** Mengimpor library utama seperti tensorflow, keras, numpy, matplotlib.pyplot, os, dan seaborn yang akan digunakan sepanjang proyek ini."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "b1ee94cf"
      },
      "source": [
        "import tensorflow as tf\n",
        "from tensorflow import keras\n",
        "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Conv2D, MaxPooling2D, Flatten, Dense, Dropout, InputLayer, BatchNormalization\n",
        "from tensorflow.keras.optimizers import Adam\n",
        "from tensorflow.keras.callbacks import ModelCheckpoint, EarlyStopping, ReduceLROnPlateau\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import os\n",
        "import zipfile\n",
        "import cv2\n",
        "import glob\n",
        "import shutil\n",
        "from mtcnn.mtcnn import MTCNN"
      ],
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "b4adda93"
      },
      "source": [
        "### **1.2. Define Configurations (Definisi Konfigurasi)**\n",
        "\n",
        "**Penjelasan:** Mendefinisikan variabel-variabel konfigurasi yang akan digunakan di seluruh notebook, termasuk path ke dataset, ukuran gambar yang akan digunakan, ukuran batch untuk training, jumlah epoch, dan jumlah kelas (mahasiswa)."
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Link Dataset Google Drive: https://drive.google.com/drive/folders/1S5mRxYOfTPAmfqqFFLfbV_D5eWj5J9ox?usp=sharing"
      ],
      "metadata": {
        "id": "sa181Rqnk7PG"
      }
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "752e8757"
      },
      "source": [
        "# Define Directory Paths (Definisi Path Direktori)\n",
        "ZIP_PATH = '/content/drive/MyDrive/Dataset/Dataset Sistem Presensi Wajah V1.0.zip' # Path to the raw zip file in Google Drive\n",
        "RAW_DATA_PATH = '/content/raw_dataset' # Directory to extract the raw dataset\n",
        "PROCESSED_PATH = '/content/processed_dataset' # Directory to save the processed (face-detected) dataset\n",
        "\n",
        "# Define Image Parameters (Definisi Parameter Gambar)\n",
        "IMG_HEIGHT = 128 # Smaller size for custom CNN from scratch\n",
        "IMG_WIDTH = 128\n",
        "CHANNELS = 3 # RGB color images\n",
        "\n",
        "# Define Training Parameters (Definisi Parameter Pelatihan)\n",
        "BATCH_SIZE = 32\n",
        "EPOCHS = 50 # Will be controlled by Early Stopping\n",
        "# NUM_CLASSES will be determined later by the data generator"
      ],
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2817c9e2"
      },
      "source": [
        "### **1.3. Extract Dataset (Ekstrak Dataset)**\n",
        "\n",
        "**Penjelasan:** Mengekstrak file dataset dari Google Drive ke lingkungan Colab agar dapat diakses sebagai direktori biasa."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "c1266c0a",
        "outputId": "9c43e92f-81bd-4a8a-a4ed-38aa7e97de0a"
      },
      "source": [
        "# Define the path to the zip file in Google Drive\n",
        "zip_path = '/content/drive/MyDrive/Dataset/Dataset Sistem Presensi Wajah V1.0.zip'\n",
        "extract_path = '/content/dataset' # Directory to extract the dataset\n",
        "\n",
        "# Create the extraction directory if it doesn't exist\n",
        "os.makedirs(extract_path, exist_ok=True)\n",
        "\n",
        "# Extract the zip file\n",
        "print(f\"Extracting {zip_path} to {extract_path}...\")\n",
        "try:\n",
        "    with zipfile.ZipFile(zip_path, 'r') as zip_ref:\n",
        "        zip_ref.extractall(extract_path)\n",
        "    print(\"Extraction complete.\")\n",
        "except FileNotFoundError:\n",
        "    print(f\"Error: Zip file not found at {zip_path}\")\n",
        "except zipfile.BadZipFile:\n",
        "    print(f\"Error: Could not open or read zip file at {zip_path}. It might be corrupted.\")\n",
        "except Exception as e:\n",
        "    print(f\"An error occurred during extraction: {e}\")\n",
        "\n",
        "# Update TRAIN_DIR and TEST_DIR to point to the extracted directories\n",
        "# Based on the previous output, the extracted content is in a subfolder\n",
        "extracted_subfolder = os.path.join(extract_path, 'Dataset Sistem Presensi Wajah V1.0')\n",
        "TRAIN_DIR = os.path.join(extracted_subfolder, 'Data Train')\n",
        "TEST_DIR = os.path.join(extracted_subfolder, 'Data Test')\n",
        "\n",
        "\n",
        "print(f\"Updated TRAIN_DIR: {TRAIN_DIR}\")\n",
        "print(f\"Updated TEST_DIR: {TEST_DIR}\")\n",
        "\n",
        "# Verify that the directories exist after extraction\n",
        "if os.path.exists(TRAIN_DIR):\n",
        "    print(f\"TRAIN_DIR exists: {TRAIN_DIR}\")\n",
        "else:\n",
        "    print(f\"Error: TRAIN_DIR not found after extraction at {TRAIN_DIR}\")\n",
        "\n",
        "if os.path.exists(TEST_DIR):\n",
        "    print(f\"TEST_DIR exists: {TEST_DIR}\")\n",
        "else:\n",
        "    print(f\"Error: TEST_DIR not found after extraction at {TEST_DIR}\")\n",
        "\n",
        "# Now it's safe to list contents if needed for verification after extraction\n",
        "print(f\"Contents of {extract_path} after extraction: {os.listdir(extract_path)}\")"
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Extracting /content/drive/MyDrive/Dataset/Dataset Sistem Presensi Wajah V1.0.zip to /content/dataset...\n",
            "Extraction complete.\n",
            "Updated TRAIN_DIR: /content/dataset/Dataset Sistem Presensi Wajah V1.0/Data Train\n",
            "Updated TEST_DIR: /content/dataset/Dataset Sistem Presensi Wajah V1.0/Data Test\n",
            "TRAIN_DIR exists: /content/dataset/Dataset Sistem Presensi Wajah V1.0/Data Train\n",
            "TEST_DIR exists: /content/dataset/Dataset Sistem Presensi Wajah V1.0/Data Test\n",
            "Contents of /content/dataset after extraction: ['Dataset Sistem Presensi Wajah V1.0']\n"
          ]
        }
      ]
    }
  ]
}