{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyMkmmjBzIEd6nE1NL+rDlZY",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/LatiefDataVisionary/deep-learning-college-task/blob/main/tasks/week_5_tasks/Task_CNN_Scratch.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5626ae91"
      },
      "source": [
        "# **Section 0: Initial Setup (Pengaturan Awal)**\n",
        "\n",
        "Bagian ini untuk melakukan instalasi library penting yang mungkin belum ada di Colab dan menghubungkan Google Drive."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "430e8d6e"
      },
      "source": [
        "## **0.1. Install Libraries (Instalasi Library)**\n",
        "\n",
        "Menginstal library `mtcnn` yang akan digunakan untuk deteksi wajah."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bf25ad7a",
        "outputId": "a6595c74-b49f-4630-a52b-4e5a0dbb506f"
      },
      "source": [
        "!pip install mtcnn opencv-python Pillow matplotlib pandas numpy scikit-learn tensorflow lz4"
      ],
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: mtcnn in /usr/local/lib/python3.12/dist-packages (1.0.0)\n",
            "Requirement already satisfied: opencv-python in /usr/local/lib/python3.12/dist-packages (4.12.0.88)\n",
            "Requirement already satisfied: Pillow in /usr/local/lib/python3.12/dist-packages (11.3.0)\n",
            "Requirement already satisfied: matplotlib in /usr/local/lib/python3.12/dist-packages (3.10.0)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.12/dist-packages (2.2.2)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.12/dist-packages (2.0.2)\n",
            "Requirement already satisfied: scikit-learn in /usr/local/lib/python3.12/dist-packages (1.6.1)\n",
            "Requirement already satisfied: tensorflow in /usr/local/lib/python3.12/dist-packages (2.19.0)\n",
            "Requirement already satisfied: lz4 in /usr/local/lib/python3.12/dist-packages (4.4.4)\n",
            "Requirement already satisfied: joblib>=1.4.2 in /usr/local/lib/python3.12/dist-packages (from mtcnn) (1.5.2)\n",
            "Requirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.12/dist-packages (from matplotlib) (1.3.3)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.12/dist-packages (from matplotlib) (0.12.1)\n",
            "Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.12/dist-packages (from matplotlib) (4.60.1)\n",
            "Requirement already satisfied: kiwisolver>=1.3.1 in /usr/local/lib/python3.12/dist-packages (from matplotlib) (1.4.9)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.12/dist-packages (from matplotlib) (25.0)\n",
            "Requirement already satisfied: pyparsing>=2.3.1 in /usr/local/lib/python3.12/dist-packages (from matplotlib) (3.2.5)\n",
            "Requirement already satisfied: python-dateutil>=2.7 in /usr/local/lib/python3.12/dist-packages (from matplotlib) (2.9.0.post0)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.12/dist-packages (from pandas) (2025.2)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.12/dist-packages (from pandas) (2025.2)\n",
            "Requirement already satisfied: scipy>=1.6.0 in /usr/local/lib/python3.12/dist-packages (from scikit-learn) (1.16.2)\n",
            "Requirement already satisfied: threadpoolctl>=3.1.0 in /usr/local/lib/python3.12/dist-packages (from scikit-learn) (3.6.0)\n",
            "Requirement already satisfied: absl-py>=1.0.0 in /usr/local/lib/python3.12/dist-packages (from tensorflow) (1.4.0)\n",
            "Requirement already satisfied: astunparse>=1.6.0 in /usr/local/lib/python3.12/dist-packages (from tensorflow) (1.6.3)\n",
            "Requirement already satisfied: flatbuffers>=24.3.25 in /usr/local/lib/python3.12/dist-packages (from tensorflow) (25.9.23)\n",
            "Requirement already satisfied: gast!=0.5.0,!=0.5.1,!=0.5.2,>=0.2.1 in /usr/local/lib/python3.12/dist-packages (from tensorflow) (0.6.0)\n",
            "Requirement already satisfied: google-pasta>=0.1.1 in /usr/local/lib/python3.12/dist-packages (from tensorflow) (0.2.0)\n",
            "Requirement already satisfied: libclang>=13.0.0 in /usr/local/lib/python3.12/dist-packages (from tensorflow) (18.1.1)\n",
            "Requirement already satisfied: opt-einsum>=2.3.2 in /usr/local/lib/python3.12/dist-packages (from tensorflow) (3.4.0)\n",
            "Requirement already satisfied: protobuf!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<6.0.0dev,>=3.20.3 in /usr/local/lib/python3.12/dist-packages (from tensorflow) (5.29.5)\n",
            "Requirement already satisfied: requests<3,>=2.21.0 in /usr/local/lib/python3.12/dist-packages (from tensorflow) (2.32.4)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.12/dist-packages (from tensorflow) (75.2.0)\n",
            "Requirement already satisfied: six>=1.12.0 in /usr/local/lib/python3.12/dist-packages (from tensorflow) (1.17.0)\n",
            "Requirement already satisfied: termcolor>=1.1.0 in /usr/local/lib/python3.12/dist-packages (from tensorflow) (3.1.0)\n",
            "Requirement already satisfied: typing-extensions>=3.6.6 in /usr/local/lib/python3.12/dist-packages (from tensorflow) (4.15.0)\n",
            "Requirement already satisfied: wrapt>=1.11.0 in /usr/local/lib/python3.12/dist-packages (from tensorflow) (2.0.0)\n",
            "Requirement already satisfied: grpcio<2.0,>=1.24.3 in /usr/local/lib/python3.12/dist-packages (from tensorflow) (1.75.1)\n",
            "Requirement already satisfied: tensorboard~=2.19.0 in /usr/local/lib/python3.12/dist-packages (from tensorflow) (2.19.0)\n",
            "Requirement already satisfied: keras>=3.5.0 in /usr/local/lib/python3.12/dist-packages (from tensorflow) (3.10.0)\n",
            "Requirement already satisfied: h5py>=3.11.0 in /usr/local/lib/python3.12/dist-packages (from tensorflow) (3.15.1)\n",
            "Requirement already satisfied: ml-dtypes<1.0.0,>=0.5.1 in /usr/local/lib/python3.12/dist-packages (from tensorflow) (0.5.3)\n",
            "Requirement already satisfied: wheel<1.0,>=0.23.0 in /usr/local/lib/python3.12/dist-packages (from astunparse>=1.6.0->tensorflow) (0.45.1)\n",
            "Requirement already satisfied: rich in /usr/local/lib/python3.12/dist-packages (from keras>=3.5.0->tensorflow) (13.9.4)\n",
            "Requirement already satisfied: namex in /usr/local/lib/python3.12/dist-packages (from keras>=3.5.0->tensorflow) (0.1.0)\n",
            "Requirement already satisfied: optree in /usr/local/lib/python3.12/dist-packages (from keras>=3.5.0->tensorflow) (0.17.0)\n",
            "Requirement already satisfied: charset_normalizer<4,>=2 in /usr/local/lib/python3.12/dist-packages (from requests<3,>=2.21.0->tensorflow) (3.4.4)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.12/dist-packages (from requests<3,>=2.21.0->tensorflow) (3.11)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.12/dist-packages (from requests<3,>=2.21.0->tensorflow) (2.5.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.12/dist-packages (from requests<3,>=2.21.0->tensorflow) (2025.10.5)\n",
            "Requirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.12/dist-packages (from tensorboard~=2.19.0->tensorflow) (3.9)\n",
            "Requirement already satisfied: tensorboard-data-server<0.8.0,>=0.7.0 in /usr/local/lib/python3.12/dist-packages (from tensorboard~=2.19.0->tensorflow) (0.7.2)\n",
            "Requirement already satisfied: werkzeug>=1.0.1 in /usr/local/lib/python3.12/dist-packages (from tensorboard~=2.19.0->tensorflow) (3.1.3)\n",
            "Requirement already satisfied: MarkupSafe>=2.1.1 in /usr/local/lib/python3.12/dist-packages (from werkzeug>=1.0.1->tensorboard~=2.19.0->tensorflow) (3.0.3)\n",
            "Requirement already satisfied: markdown-it-py>=2.2.0 in /usr/local/lib/python3.12/dist-packages (from rich->keras>=3.5.0->tensorflow) (4.0.0)\n",
            "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /usr/local/lib/python3.12/dist-packages (from rich->keras>=3.5.0->tensorflow) (2.19.2)\n",
            "Requirement already satisfied: mdurl~=0.1 in /usr/local/lib/python3.12/dist-packages (from markdown-it-py>=2.2.0->rich->keras>=3.5.0->tensorflow) (0.1.2)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8ca0d611"
      },
      "source": [
        "## **0.2. Mount Google Drive (Menghubungkan Google Drive)**\n",
        "\n",
        "Menghubungkan notebook dengan Google Drive agar dapat mengakses dataset."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bce83f40",
        "outputId": "52356c5a-f439-4923-8b9d-2ddf3d2b54c5"
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4c13c338"
      },
      "source": [
        "## **Section 1: Import Libraries and Environment Setup (Impor Library dan Pengaturan Lingkungan)**\n",
        "\n",
        "**Penjelasan:** Di sini kita akan mengimpor semua modul dan library yang dibutuhkan untuk keseluruhan proyek serta mendefinisikan variabel-variabel global seperti path direktori, ukuran gambar, dan parameter training."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9576c1c5"
      },
      "source": [
        "### **1.1. Import Core Libraries (Impor Library Utama)**\n",
        "\n",
        "**Penjelasan:** Mengimpor library utama seperti tensorflow, keras, numpy, matplotlib.pyplot, os, dan seaborn yang akan digunakan sepanjang proyek ini."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "b1ee94cf"
      },
      "source": [
        "import tensorflow as tf\n",
        "from tensorflow import keras\n",
        "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Conv2D, MaxPooling2D, Flatten, Dense, Dropout, InputLayer, BatchNormalization\n",
        "from tensorflow.keras.optimizers import Adam\n",
        "from tensorflow.keras.callbacks import ModelCheckpoint, EarlyStopping, ReduceLROnPlateau\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import os\n",
        "import zipfile\n",
        "import cv2\n",
        "import glob\n",
        "import shutil\n",
        "from mtcnn.mtcnn import MTCNN"
      ],
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "b4adda93"
      },
      "source": [
        "### **1.2. Define Configurations (Definisi Konfigurasi)**\n",
        "\n",
        "**Penjelasan:** Mendefinisikan variabel-variabel konfigurasi yang akan digunakan di seluruh notebook, termasuk path ke dataset, ukuran gambar yang akan digunakan, ukuran batch untuk training, jumlah epoch, dan jumlah kelas (mahasiswa)."
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Link Dataset Google Drive: https://drive.google.com/drive/folders/1S5mRxYOfTPAmfqqFFLfbV_D5eWj5J9ox?usp=sharing"
      ],
      "metadata": {
        "id": "sa181Rqnk7PG"
      }
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "752e8757"
      },
      "source": [
        "# Define Directory Paths (Definisi Path Direktori)\n",
        "ZIP_PATH = '/content/drive/MyDrive/Dataset/Dataset Sistem Presensi Wajah V1.0.zip' # Path to the raw zip file in Google Drive\n",
        "RAW_DATA_PATH = '/content/raw_dataset' # Directory to extract the raw dataset\n",
        "PROCESSED_PATH = '/content/processed_dataset' # Directory to save the processed (face-detected) dataset\n",
        "\n",
        "# Define Image Parameters (Definisi Parameter Gambar)\n",
        "IMG_HEIGHT = 128 # Smaller size for custom CNN from scratch\n",
        "IMG_WIDTH = 128\n",
        "CHANNELS = 3 # RGB color images\n",
        "\n",
        "# Define Training Parameters (Definisi Parameter Pelatihan)\n",
        "BATCH_SIZE = 32\n",
        "EPOCHS = 50 # Will be controlled by Early Stopping\n",
        "# NUM_CLASSES will be determined later by the data generator"
      ],
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2817c9e2"
      },
      "source": [
        "### **1.3. Extract Dataset (Ekstrak Dataset)**\n",
        "\n",
        "**Penjelasan:** Mengekstrak file dataset dari Google Drive ke lingkungan Colab agar dapat diakses sebagai direktori biasa."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "c1266c0a",
        "outputId": "9c43e92f-81bd-4a8a-a4ed-38aa7e97de0a"
      },
      "source": [
        "# Define the path to the zip file in Google Drive\n",
        "zip_path = '/content/drive/MyDrive/Dataset/Dataset Sistem Presensi Wajah V1.0.zip'\n",
        "extract_path = '/content/dataset' # Directory to extract the dataset\n",
        "\n",
        "# Create the extraction directory if it doesn't exist\n",
        "os.makedirs(extract_path, exist_ok=True)\n",
        "\n",
        "# Extract the zip file\n",
        "print(f\"Extracting {zip_path} to {extract_path}...\")\n",
        "try:\n",
        "    with zipfile.ZipFile(zip_path, 'r') as zip_ref:\n",
        "        zip_ref.extractall(extract_path)\n",
        "    print(\"Extraction complete.\")\n",
        "except FileNotFoundError:\n",
        "    print(f\"Error: Zip file not found at {zip_path}\")\n",
        "except zipfile.BadZipFile:\n",
        "    print(f\"Error: Could not open or read zip file at {zip_path}. It might be corrupted.\")\n",
        "except Exception as e:\n",
        "    print(f\"An error occurred during extraction: {e}\")\n",
        "\n",
        "# Update TRAIN_DIR and TEST_DIR to point to the extracted directories\n",
        "# Based on the previous output, the extracted content is in a subfolder\n",
        "extracted_subfolder = os.path.join(extract_path, 'Dataset Sistem Presensi Wajah V1.0')\n",
        "TRAIN_DIR = os.path.join(extracted_subfolder, 'Data Train')\n",
        "TEST_DIR = os.path.join(extracted_subfolder, 'Data Test')\n",
        "\n",
        "\n",
        "print(f\"Updated TRAIN_DIR: {TRAIN_DIR}\")\n",
        "print(f\"Updated TEST_DIR: {TEST_DIR}\")\n",
        "\n",
        "# Verify that the directories exist after extraction\n",
        "if os.path.exists(TRAIN_DIR):\n",
        "    print(f\"TRAIN_DIR exists: {TRAIN_DIR}\")\n",
        "else:\n",
        "    print(f\"Error: TRAIN_DIR not found after extraction at {TRAIN_DIR}\")\n",
        "\n",
        "if os.path.exists(TEST_DIR):\n",
        "    print(f\"TEST_DIR exists: {TEST_DIR}\")\n",
        "else:\n",
        "    print(f\"Error: TEST_DIR not found after extraction at {TEST_DIR}\")\n",
        "\n",
        "# Now it's safe to list contents if needed for verification after extraction\n",
        "print(f\"Contents of {extract_path} after extraction: {os.listdir(extract_path)}\")"
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Extracting /content/drive/MyDrive/Dataset/Dataset Sistem Presensi Wajah V1.0.zip to /content/dataset...\n",
            "Extraction complete.\n",
            "Updated TRAIN_DIR: /content/dataset/Dataset Sistem Presensi Wajah V1.0/Data Train\n",
            "Updated TEST_DIR: /content/dataset/Dataset Sistem Presensi Wajah V1.0/Data Test\n",
            "TRAIN_DIR exists: /content/dataset/Dataset Sistem Presensi Wajah V1.0/Data Train\n",
            "TEST_DIR exists: /content/dataset/Dataset Sistem Presensi Wajah V1.0/Data Test\n",
            "Contents of /content/dataset after extraction: ['Dataset Sistem Presensi Wajah V1.0']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5d31ee2a"
      },
      "source": [
        "## **Section 2: Advanced Preprocessing - Face Detection and Cropping (Preprocessing Lanjutan - Deteksi dan Pemotongan Wajah)**\n",
        "\n",
        "**Penjelasan:** Ini adalah tahap paling krusial dan merupakan upgrade utama. Kita akan memproses seluruh dataset mentah sekali jalan. Tujuannya adalah mendeteksi wajah di setiap gambar, memotongnya, dan menyimpannya ke struktur direktori baru yang bersih dan siap pakai. Proses ini menyelesaikan masalah distorsi aspect ratio dan noise latar belakang."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5b73bfdd"
      },
      "source": [
        "### **2.1. Unzip Raw Dataset (Ekstrak Dataset Mentah)**\n",
        "\n",
        "**Penjelasan:** Mengekstrak file dataset mentah dari lokasi ZIP_PATH ke direktori RAW_DATA_PATH agar dapat diakses sebagai file gambar."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "f255fac3",
        "outputId": "3a0b77d2-c715-443c-a1f9-6c0d4882cbd1"
      },
      "source": [
        "# Create the raw data extraction directory if it doesn't exist\n",
        "os.makedirs(RAW_DATA_PATH, exist_ok=True)\n",
        "\n",
        "# Extract the zip file to the raw data path\n",
        "print(f\"Extracting {ZIP_PATH} to {RAW_DATA_PATH}...\")\n",
        "try:\n",
        "    with zipfile.ZipFile(ZIP_PATH, 'r') as zip_ref:\n",
        "        zip_ref.extractall(RAW_DATA_PATH)\n",
        "    print(\"Extraction complete.\")\n",
        "except FileNotFoundError:\n",
        "    print(f\"Error: Zip file not found at {ZIP_PATH}\")\n",
        "except zipfile.BadZipFile:\n",
        "    print(f\"Error: Could not open or read zip file at {ZIP_PATH}. It might be corrupted.\")\n",
        "except Exception as e:\n",
        "    print(f\"An error occurred during extraction: {e}\")\n",
        "\n",
        "# Verify contents of the extracted raw data directory\n",
        "print(f\"Contents of {RAW_DATA_PATH} after extraction: {os.listdir(RAW_DATA_PATH)}\")\n",
        "\n",
        "# Determine the actual path to the raw image files inside the extracted folder\n",
        "# Assuming the zip contains a single main folder\n",
        "extracted_items = os.listdir(RAW_DATA_PATH)\n",
        "if len(extracted_items) == 1 and os.path.isdir(os.path.join(RAW_DATA_PATH, extracted_items[0])):\n",
        "    ACTUAL_RAW_DATA_ROOT = os.path.join(RAW_DATA_PATH, extracted_items[0])\n",
        "else:\n",
        "    # If structure is different, you might need to adjust this\n",
        "    ACTUAL_RAW_DATA_ROOT = RAW_DATA_PATH\n",
        "    print(\"Warning: Extracted data structure is not a single subfolder. Assuming raw images are directly in RAW_DATA_PATH.\")\n",
        "\n",
        "print(f\"Actual root directory for raw images: {ACTUAL_RAW_DATA_ROOT}\")\n",
        "\n",
        "# List a few files to confirm\n",
        "raw_image_files = glob.glob(os.path.join(ACTUAL_RAW_DATA_ROOT, '*.*'))\n",
        "print(f\"Found {len(raw_image_files)} raw image files.\")\n",
        "if len(raw_image_files) > 5:\n",
        "    print(\"First 5 raw files:\", raw_image_files[:5])\n",
        "elif len(raw_image_files) > 0:\n",
        "     print(\"Raw files:\", raw_image_files)\n",
        "else:\n",
        "    print(\"No raw image files found. Check ZIP_PATH and extraction process.\")"
      ],
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Extracting /content/drive/MyDrive/Dataset/Dataset Sistem Presensi Wajah V1.0.zip to /content/raw_dataset...\n",
            "Extraction complete.\n",
            "Contents of /content/raw_dataset after extraction: ['Dataset Sistem Presensi Wajah V1.0']\n",
            "Actual root directory for raw images: /content/raw_dataset/Dataset Sistem Presensi Wajah V1.0\n",
            "Found 0 raw image files.\n",
            "No raw image files found. Check ZIP_PATH and extraction process.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "b960b805"
      },
      "source": [
        "### **2.2. Initialize Face Detector (Inisialisasi Detektor Wajah)**\n",
        "\n",
        "**Penjelasan:** Menginisialisasi model MTCNN yang akan digunakan untuk mendeteksi wajah pada setiap gambar."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 321
        },
        "id": "7f7759e2",
        "outputId": "fed5ee18-e5dc-4869-e29b-84159e1970dc"
      },
      "source": [
        "# Initialize MTCNN detector\n",
        "detector = MTCNN()\n",
        "print(\"MTCNN detector initialized.\")"
      ],
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "error",
          "ename": "ValueError",
          "evalue": "LZ4 is not installed. Install it with pip: https://python-lz4.readthedocs.io/",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
            "\u001b[0;32m/tmp/ipython-input-2378853677.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# Initialize MTCNN detector\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mdetector\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mMTCNN\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"MTCNN detector initialized.\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/mtcnn/mtcnn.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, stages, device)\u001b[0m\n\u001b[1;32m     58\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     59\u001b[0m         \u001b[0;31m# Instantiate stages if necessary (can pass already instantiated stages too)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 60\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_stages\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mstage\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstage\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtype\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0mstage\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mstage\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mstages\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     61\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_device\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdevice\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     62\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/mtcnn/stages/stage_pnet.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, stage_name, stage_id, weights)\u001b[0m\n\u001b[1;32m     57\u001b[0m         \u001b[0mmodel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mPNet\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     58\u001b[0m         \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbuild\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# Building the model (no need to specify input shape if default is provided)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 59\u001b[0;31m         \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mset_weights\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mload_weights\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mweights\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# Load pre-trained weights\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     60\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     61\u001b[0m         \u001b[0msuper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__init__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstage_name\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mstage_name\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstage_id\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mstage_id\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/mtcnn/utils/tensorflow.py\u001b[0m in \u001b[0;36mload_weights\u001b[0;34m(weights_name)\u001b[0m\n\u001b[1;32m     47\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mpath\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mpaths\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     48\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexists\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m  \u001b[0;31m# First checks the local filesystem\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 49\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mjoblib\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     50\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     51\u001b[0m     \u001b[0;31m# If no file is found, raise an error\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/joblib/numpy_pickle.py\u001b[0m in \u001b[0;36mload\u001b[0;34m(filename, mmap_mode, ensure_native_byte_order)\u001b[0m\n\u001b[1;32m    734\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    735\u001b[0m         \u001b[0;32mwith\u001b[0m \u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilename\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"rb\"\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 736\u001b[0;31m             with _validate_fileobject_and_memmap(f, filename, mmap_mode) as (\n\u001b[0m\u001b[1;32m    737\u001b[0m                 \u001b[0mfobj\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    738\u001b[0m                 \u001b[0mvalidated_mmap_mode\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.12/contextlib.py\u001b[0m in \u001b[0;36m__enter__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    135\u001b[0m         \u001b[0;32mdel\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfunc\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    136\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 137\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mnext\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgen\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    138\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mStopIteration\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    139\u001b[0m             \u001b[0;32mraise\u001b[0m \u001b[0mRuntimeError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"generator didn't yield\"\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/joblib/numpy_pickle_utils.py\u001b[0m in \u001b[0;36m_validate_fileobject_and_memmap\u001b[0;34m(fileobj, filename, mmap_mode)\u001b[0m\n\u001b[1;32m    175\u001b[0m             \u001b[0;31m# correct decompressor file object, wrapped in a buffer.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    176\u001b[0m             \u001b[0mcompressor_wrapper\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_COMPRESSORS\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mcompressor\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 177\u001b[0;31m             \u001b[0minst\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcompressor_wrapper\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdecompressor_file\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfileobj\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    178\u001b[0m             \u001b[0mfileobj\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_buffered_read_file\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minst\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    179\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/joblib/compressor.py\u001b[0m in \u001b[0;36mdecompressor_file\u001b[0;34m(self, fileobj)\u001b[0m\n\u001b[1;32m    217\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mdecompressor_file\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfileobj\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    218\u001b[0m         \u001b[0;34m\"\"\"Returns an instance of a decompressor file object.\"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 219\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_check_versions\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    220\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfileobj_factory\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfileobj\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"rb\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    221\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/joblib/compressor.py\u001b[0m in \u001b[0;36m_check_versions\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    200\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_check_versions\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    201\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mlz4\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 202\u001b[0;31m             \u001b[0;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mLZ4_NOT_INSTALLED_ERROR\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    203\u001b[0m         \u001b[0mlz4_version\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlz4\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__version__\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    204\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mlz4_version\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstartswith\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"v\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mValueError\u001b[0m: LZ4 is not installed. Install it with pip: https://python-lz4.readthedocs.io/"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "f15b91bc"
      },
      "source": [
        "### **2.3. Prepare Processed Directory Structure (Siapkan Struktur Direktori Hasil Proses)**\n",
        "\n",
        "**Penjelasan:** Membuat struktur direktori baru di PROCESSED_PATH untuk menyimpan gambar wajah yang sudah dideteksi dan dipotong. Struktur ini akan memiliki sub-folder untuk data training dan testing, dan di dalamnya akan ada sub-folder untuk setiap kelas (berdasarkan NIM)."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "a234a96f"
      },
      "source": [
        "# Clean up and create the processed data directories\n",
        "if os.path.exists(PROCESSED_PATH):\n",
        "    print(f\"Removing existing processed data directory: {PROCESSED_PATH}\")\n",
        "    shutil.rmtree(PROCESSED_PATH)\n",
        "\n",
        "os.makedirs(PROCESSED_PATH)\n",
        "os.makedirs(os.path.join(PROCESSED_PATH, 'train'))\n",
        "os.makedirs(os.path.join(PROCESSED_PATH, 'test'))\n",
        "print(f\"Created processed data directories: {PROCESSED_PATH}/train and {PROCESSED_PATH}/test\")\n",
        "\n",
        "# Get unique class names (NIMs) from raw filenames\n",
        "# Assuming raw files are directly under ACTUAL_RAW_DATA_ROOT\n",
        "raw_filenames = os.listdir(ACTUAL_RAW_DATA_ROOT)\n",
        "# Filter for image files if necessary (e.g., ends with .jpg, .png)\n",
        "image_filenames = [f for f in raw_filenames if f.lower().endswith(('.jpg', '.jpeg', '.png'))]\n",
        "\n",
        "# Extract NIM (first 10 digits) as class labels\n",
        "class_names = sorted(list(set([f[:10] for f in image_filenames if len(f) >= 10])))\n",
        "\n",
        "if not class_names:\n",
        "    print(\"Error: No class names (NIMs) extracted from filenames. Check file naming convention and ACTUAL_RAW_DATA_ROOT.\")\n",
        "else:\n",
        "    print(f\"Found {len(class_names)} unique classes (NIMs).\")\n",
        "    # Create sub-folders for each class in train and test directories\n",
        "    for class_name in class_names:\n",
        "        os.makedirs(os.path.join(PROCESSED_PATH, 'train', class_name), exist_ok=True)\n",
        "        os.makedirs(os.path.join(PROCESSED_PATH, 'test', class_name), exist_ok=True)\n",
        "    print(\"Created class sub-folders in processed train and test directories.\")\n",
        "\n",
        "# Store class_names for later use\n",
        "CLASS_NAMES = class_names"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "30653e48"
      },
      "source": [
        "### **2.4. Run the Face Detection & Cropping Pipeline (Jalankan Pipeline Deteksi & Pemotongan Wajah)**\n",
        "\n",
        "**Penjelasan:** Membuat dan menjalankan fungsi untuk mendeteksi wajah di setiap gambar mentah, memotongnya, dan menyimpannya ke struktur direktori yang sudah disiapkan di PROCESSED_PATH. Data akan dibagi secara manual menjadi training dan testing (misal: 80% train, 20% test per kelas)."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0f824f70"
      },
      "source": [
        "def process_and_save_faces(raw_data_root_dir, processed_train_dir, processed_test_dir, detector, img_width, img_height, split_ratio=0.8):\n",
        "    \"\"\"\n",
        "    Processes raw images: detects faces, crops them, resizes, and saves to\n",
        "    processed train/test directories based on NIM from filename.\n",
        "\n",
        "    Args:\n",
        "        raw_data_root_dir (str): Directory containing all raw images.\n",
        "        processed_train_dir (str): Destination directory for processed training images.\n",
        "        processed_test_dir (str): Destination directory for processed testing images.\n",
        "        detector (MTCNN): Initialized MTCNN face detector.\n",
        "        img_width (int): Target width for processed images.\n",
        "        img_height (int): Target height for processed images.\n",
        "        split_ratio (float): Ratio of data to use for training (e.g., 0.8 for 80% train).\n",
        "    \"\"\"\n",
        "    print(f\"Starting face detection and cropping pipeline from {raw_data_root_dir}...\")\n",
        "\n",
        "    # Group files by class (NIM)\n",
        "    class_files = {}\n",
        "    all_raw_files = glob.glob(os.path.join(raw_data_root_dir, '*.*'))\n",
        "    for filepath in all_raw_files:\n",
        "        filename = os.path.basename(filepath)\n",
        "        if len(filename) >= 10:\n",
        "            nim = filename[:10]\n",
        "            if nim in CLASS_NAMES: # Ensure NIM is one of the identified classes\n",
        "                 if nim not in class_files:\n",
        "                     class_files[nim] = []\n",
        "                 class_files[nim].append(filepath)\n",
        "\n",
        "    total_processed = 0\n",
        "    total_skipped = 0\n",
        "\n",
        "    for nim, files in class_files.items():\n",
        "        print(f\"Processing class {nim} with {len(files)} images...\")\n",
        "        # Shuffle files for random train/test split\n",
        "        np.random.shuffle(files)\n",
        "        split_index = int(len(files) * split_ratio)\n",
        "\n",
        "        train_files = files[:split_index]\n",
        "        test_files = files[split_index:]\n",
        "\n",
        "        print(f\"  - {len(train_files)} for training, {len(test_files)} for testing.\")\n",
        "\n",
        "        # Process training files\n",
        "        for i, filepath in enumerate(train_files):\n",
        "            filename = os.path.basename(filepath)\n",
        "            try:\n",
        "                image = cv2.imread(filepath)\n",
        "                if image is None:\n",
        "                    print(f\"Warning: Could not read image file: {filepath}. Skipping.\")\n",
        "                    total_skipped += 1\n",
        "                    continue\n",
        "\n",
        "                # Convert BGR to RGB (MTCNN expects RGB)\n",
        "                image_rgb = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n",
        "\n",
        "                # Detect faces\n",
        "                results = detector.detect_faces(image_rgb)\n",
        "\n",
        "                if results:\n",
        "                    # Get the first detected face (assuming one main face per image)\n",
        "                    x, y, width, height = results[0]['box']\n",
        "\n",
        "                    # Add margin (adjust as needed)\n",
        "                    margin_x = int(width * 0.2)\n",
        "                    margin_y = int(height * 0.2)\n",
        "                    x1 = max(0, x - margin_x)\n",
        "                    y1 = max(0, y - margin_y)\n",
        "                    x2 = min(image.shape[1], x + width + margin_x)\n",
        "                    y2 = min(image.shape[0], y + height + margin_y)\n",
        "\n",
        "                    # Crop the face with margin\n",
        "                    face_crop = image[y1:y2, x1:x2]\n",
        "\n",
        "                    # Resize the cropped face to target size\n",
        "                    face_resized = cv2.resize(face_crop, (img_width, img_height))\n",
        "\n",
        "                    # Save the processed face image to the training directory\n",
        "                    dest_filepath = os.path.join(processed_train_dir, nim, filename)\n",
        "                    cv2.imwrite(dest_filepath, face_resized)\n",
        "                    total_processed += 1\n",
        "                else:\n",
        "                    print(f\"Warning: No face detected in {filepath}. Skipping.\")\n",
        "                    total_skipped += 1\n",
        "\n",
        "            except Exception as e:\n",
        "                print(f\"Error processing {filepath}: {e}. Skipping.\")\n",
        "                total_skipped += 1\n",
        "\n",
        "        # Process testing files\n",
        "        for i, filepath in enumerate(test_files):\n",
        "             filename = os.path.basename(filepath)\n",
        "             try:\n",
        "                image = cv2.imread(filepath)\n",
        "                if image is None:\n",
        "                    print(f\"Warning: Could not read image file: {filepath}. Skipping.\")\n",
        "                    total_skipped += 1\n",
        "                    continue\n",
        "\n",
        "                # Convert BGR to RGB (MTCNN expects RGB)\n",
        "                image_rgb = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n",
        "\n",
        "                # Detect faces\n",
        "                results = detector.detect_faces(image_rgb)\n",
        "\n",
        "                if results:\n",
        "                    # Get the first detected face\n",
        "                    x, y, width, height = results[0]['box']\n",
        "\n",
        "                    # Add margin\n",
        "                    margin_x = int(width * 0.2)\n",
        "                    margin_y = int(height * 0.2)\n",
        "                    x1 = max(0, x - margin_x)\n",
        "                    y1 = max(0, y - margin_y)\n",
        "                    x2 = min(image.shape[1], x + width + margin_x)\n",
        "                    y2 = min(image.shape[0], y + height + margin_y)\n",
        "\n",
        "\n",
        "                    # Crop the face with margin\n",
        "                    face_crop = image[y1:y2, x1:x2]\n",
        "\n",
        "                    # Resize the cropped face to target size\n",
        "                    face_resized = cv2.resize(face_crop, (img_width, img_height))\n",
        "\n",
        "                    # Save the processed face image to the testing directory\n",
        "                    dest_filepath = os.path.join(processed_test_dir, nim, filename)\n",
        "                    cv2.imwrite(dest_filepath, face_resized)\n",
        "                    total_processed += 1\n",
        "                else:\n",
        "                    print(f\"Warning: No face detected in {filepath}. Skipping.\")\n",
        "                    total_skipped += 1\n",
        "\n",
        "             except Exception as e:\n",
        "                print(f\"Error processing {filepath}: {e}. Skipping.\")\n",
        "                total_skipped += 1\n",
        "\n",
        "\n",
        "    print(\"\\nFace detection and cropping pipeline finished.\")\n",
        "    print(f\"Total images processed and saved: {total_processed}\")\n",
        "    print(f\"Total images skipped (no face detected or error): {total_skipped}\")\n",
        "\n",
        "# Run the pipeline\n",
        "# Ensure ACTUAL_RAW_DATA_ROOT is correctly determined in step 2.1\n",
        "if 'ACTUAL_RAW_DATA_ROOT' in globals() and os.path.exists(ACTUAL_RAW_DATA_ROOT):\n",
        "    process_and_save_faces(ACTUAL_RAW_DATA_ROOT,\n",
        "                           os.path.join(PROCESSED_PATH, 'train'),\n",
        "                           os.path.join(PROCESSED_PATH, 'test'),\n",
        "                           detector,\n",
        "                           IMG_WIDTH, IMG_HEIGHT)\n",
        "else:\n",
        "    print(\"Error: ACTUAL_RAW_DATA_ROOT is not set or does not exist. Cannot run processing pipeline.\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4f987bd9"
      },
      "source": [
        "### **2.5. Verify Processed Dataset (Verifikasi Dataset Hasil Proses)**\n",
        "\n",
        "**Penjelasan:** Memeriksa jumlah gambar di direktori training dan testing yang sudah diproses untuk memastikan bahwa pipeline deteksi dan pemotongan wajah berjalan dengan sukses dan data tersimpan dengan benar."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "455ac6d0"
      },
      "source": [
        "# Function to count images in a directory, including subdirectories\n",
        "def count_images_in_directory(directory):\n",
        "    count = 0\n",
        "    if not os.path.exists(directory):\n",
        "        return 0\n",
        "    for root, _, files in os.walk(directory):\n",
        "        for file in files:\n",
        "            if file.lower().endswith(('.jpg', '.jpeg', '.png')):\n",
        "                count += 1\n",
        "    return count\n",
        "\n",
        "# Count images in processed train and test directories\n",
        "train_count = count_images_in_directory(os.path.join(PROCESSED_PATH, 'train'))\n",
        "test_count = count_images_in_directory(os.path.join(PROCESSED_PATH, 'test'))\n",
        "\n",
        "print(f\"Number of processed images in training directory ({os.path.join(PROCESSED_PATH, 'train')}): {train_count}\")\n",
        "print(f\"Number of processed images in testing directory ({os.path.join(PROCESSED_PATH, 'test')}): {test_count}\")\n",
        "\n",
        "if train_count == 0 or test_count == 0:\n",
        "    print(\"Warning: No processed images found in one or both directories. Check the processing pipeline and file paths.\")\n",
        "else:\n",
        "    print(\"Processed dataset structure verified.\")"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}