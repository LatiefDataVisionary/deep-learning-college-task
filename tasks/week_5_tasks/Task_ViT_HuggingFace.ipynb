{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "toc_visible": true,
      "gpuType": "T4",
      "authorship_tag": "ABX9TyNebiwyR7eXi9XSUHx1Ue+8",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/LatiefDataVisionary/deep-learning-college-task/blob/main/tasks/week_5_tasks/Task_ViT_HuggingFace.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2a2672bf"
      },
      "source": [
        "## **Section 0: Initial Setup (Pengaturan Awal)**\n",
        "\n",
        "**Penjelasan:** Bagian ini untuk melakukan instalasi library penting yang mungkin belum ada di Colab dan menghubungkan Google Drive."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "f7887a22"
      },
      "source": [
        "### **0.1. Install Libraries (Instalasi Library)**\n",
        "\n",
        "**Penjelasan:** Menginstal library tambahan yang mungkin diperlukan, yaitu `mtcnn` yang merupakan library kunci untuk deteksi wajah."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "a981b556",
        "outputId": "c1ec568f-879a-432a-acd4-0b24206ade24"
      },
      "source": [
        "# Install mtcnn library\n",
        "!pip install opencv"
      ],
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[31mERROR: Could not find a version that satisfies the requirement opencv (from versions: none)\u001b[0m\u001b[31m\n",
            "\u001b[0m\u001b[31mERROR: No matching distribution found for opencv\u001b[0m\u001b[31m\n",
            "\u001b[0m"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0cd8f58a"
      },
      "source": [
        "### **0.2. Mount Google Drive (Menghubungkan Google Drive**)\n",
        "\n",
        "**Penjelasan:** Menghubungkan notebook ini dengan akun Google Drive Anda. Ini diperlukan agar notebook dapat membaca file dataset gambar yang telah Anda simpan di Google Drive. Setelah menjalankan sel ini, ikuti instruksi otorisasi yang muncul."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7c510c84",
        "outputId": "94f6a476-7318-4f5f-8a94-cfb3e220eac8"
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "647d31f8"
      },
      "source": [
        "## **Section 1: Import Libraries and Environment Setup (Impor Library dan Pengaturan Lingkungan)**\n",
        "\n",
        "**Penjelasan:** Mengimpor semua modul yang dibutuhkan dan mendefinisikan variabel-variabel global, termasuk path untuk data mentah dan data yang akan diproses."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "29c5d6a5"
      },
      "source": [
        "### 1.1. Import Core Libraries (Impor Library Utama)\n",
        "\n",
        "**Penjelasan:** Mengimpor library utama seperti tensorflow, keras, numpy, matplotlib, os, zipfile, cv2 (OpenCV), glob, shutil yang akan digunakan sepanjang proyek ini."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2f552c82"
      },
      "source": [
        "import tensorflow as tf\n",
        "from tensorflow import keras\n",
        "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Conv2D, MaxPooling2D, Flatten, Dense, Dropout, BatchNormalization, InputLayer\n",
        "from tensorflow.keras.optimizers import Adam\n",
        "from tensorflow.keras.callbacks import ModelCheckpoint, EarlyStopping\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import os\n",
        "import zipfile\n",
        "import cv2 # Import OpenCV\n",
        "import glob # To list files\n",
        "import shutil # To manage directories\n",
        "import random\n",
        "from sklearn.metrics import classification_report, confusion_matrix\n",
        "import seaborn as sns # For confusion matrix visualization"
      ],
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1e572564"
      },
      "source": [
        "### **1.2. Define Configurations (Definisi Konfigurasi)**\n",
        "\n",
        "**Penjelasan:** Mendefinisikan variabel-variabel konfigurasi yang akan digunakan di seluruh notebook, termasuk path ke dataset, ukuran gambar yang akan digunakan, ukuran batch untuk training, jumlah epoch, dan jumlah kelas (mahasiswa)."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "48aff23b"
      },
      "source": [
        "# Define Directory Paths (Definisi Path Direktori)\n",
        "# Path ke file zip di Google Drive\n",
        "ZIP_PATH = '/content/drive/MyDrive/Dataset/Dataset Sistem Presensi Wajah V1.0.zip'\n",
        "# Path untuk mengekstrak data mentah (sebelum deteksi wajah)\n",
        "RAW_DATA_PATH = '/content/raw_dataset'\n",
        "# Path untuk menyimpan dataset yang sudah bersih (setelah deteksi wajah)\n",
        "PROCESSED_PATH = '/content/processed_dataset'\n",
        "\n",
        "# Define Image Parameters (Definisi Parameter Gambar)\n",
        "IMG_HEIGHT = 128 # Ukuran yang lebih kecil cocok untuk model dari dasar\n",
        "IMG_WIDTH = 128\n",
        "CHANNELS = 3 # RGB color images\n",
        "\n",
        "# Define Training Parameters (Definisi Parameter Pelatihan)\n",
        "BATCH_SIZE = 32\n",
        "EPOCHS = 50 # Will be controlled by Early Stopping\n",
        "# NUM_CLASSES will be determined by the data generator later\n",
        "NUM_CLASSES = None # Placeholder"
      ],
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "1mmwlN92LofO"
      },
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8f5bd934"
      },
      "source": [
        "## **Section 2: Advanced Preprocessing - Face Detection and Cropping (Preprocessing Lanjutan - Deteksi dan Pemotongan Wajah)**\n",
        "\n",
        "**Penjelasan:** Ini adalah tahap paling krusial dan merupakan upgrade utama. Kita akan memproses seluruh dataset mentah sekali jalan. Tujuannya adalah mendeteksi wajah di setiap gambar, memotongnya, dan menyimpannya ke struktur direktori baru yang bersih dan siap pakai. Ini menyelesaikan masalah distorsi aspect ratio dan noise latar belakang menggunakan deteksi wajah berbasis OpenCV Haar Cascades."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "47fc9757"
      },
      "source": [
        "### **2.1. Unzip Raw Dataset (Ekstrak Dataset Mentah)**\n",
        "\n",
        "**Penjelasan:** Kode untuk mengekstrak file .zip ke RAW_DATA_PATH."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "454e6dae",
        "outputId": "f85f0623-e28b-42c5-d844-caca3b588dc5"
      },
      "source": [
        "print(f\"Extracting {ZIP_PATH} to {RAW_DATA_PATH}...\")\n",
        "try:\n",
        "    with zipfile.ZipFile(ZIP_PATH, 'r') as zip_ref:\n",
        "        zip_ref.extractall(RAW_DATA_PATH)\n",
        "    print(\"Extraction complete.\")\n",
        "except FileNotFoundError:\n",
        "    print(f\"Error: Zip file not found at {ZIP_PATH}\")\n",
        "except zipfile.BadZipFile:\n",
        "    print(f\"Error: Could not open or read zip file at {ZIP_PATH}. It might be corrupted.\")\n",
        "except Exception as e:\n",
        "    print(f\"An error occurred during extraction: {e}\")\n",
        "\n",
        "# Verify extraction\n",
        "if os.path.exists(RAW_DATA_PATH):\n",
        "    print(f\"\\nContents of {RAW_DATA_PATH} after extraction: {os.listdir(RAW_DATA_PATH)}\")\n",
        "else:\n",
        "    print(f\"\\nError: Raw data directory not found after extraction at {RAW_DATA_PATH}\")\n",
        "\n",
        "# Assuming the extracted content is in a subfolder within RAW_DATA_PATH\n",
        "# Let's find the actual folder containing the images\n",
        "raw_image_folder = None\n",
        "for item in os.listdir(RAW_DATA_PATH):\n",
        "    item_path = os.path.join(RAW_DATA_PATH, item)\n",
        "    if os.path.isdir(item_path):\n",
        "        # Simple check: if it contains subfolders or files ending with .jpg\n",
        "        if any(os.path.isdir(os.path.join(item_path, sub_item)) for sub_item in os.listdir(item_path)) or \\\n",
        "           any(sub_item.endswith('.jpg') for sub_item in os.listdir(item_path)):\n",
        "           raw_image_folder = item_path\n",
        "           break\n",
        "\n",
        "if raw_image_folder:\n",
        "    print(f\"\\nIdentified raw image folder: {raw_image_folder}\")\n",
        "    # Update RAW_DATA_PATH to point directly to the folder containing the images\n",
        "    RAW_DATA_PATH = raw_image_folder\n",
        "    print(f\"Updated RAW_DATA_PATH: {RAW_DATA_PATH}\")\n",
        "else:\n",
        "    print(\"\\nWarning: Could not identify the main folder containing raw images within the extracted directory.\")\n",
        "    print(\"Please manually inspect the extracted contents and update RAW_DATA_PATH if necessary.\")"
      ],
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Extracting /content/drive/MyDrive/Dataset/Dataset Sistem Presensi Wajah V1.0.zip to /content/raw_dataset...\n",
            "Extraction complete.\n",
            "\n",
            "Contents of /content/raw_dataset after extraction: ['Dataset Sistem Presensi Wajah V1.0']\n",
            "\n",
            "Identified raw image folder: /content/raw_dataset/Dataset Sistem Presensi Wajah V1.0\n",
            "Updated RAW_DATA_PATH: /content/raw_dataset/Dataset Sistem Presensi Wajah V1.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "e827b231"
      },
      "source": [
        "### **2.2. Initialize Face Detector (OpenCV Haar Cascade)**\n",
        "\n",
        "**Penjelasan:** Memuat file Haar Cascade pre-trained untuk deteksi wajah dari OpenCV. Kita akan menggunakan detektor wajah frontal default."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "30b5147d",
        "outputId": "ebb9d03d-3ea4-4a6e-d95f-ec95bc5f37b6"
      },
      "source": [
        "# Remove the Haar Cascade XML file loading as we are no longer using it\n",
        "# haar_cascade_path = cv2.data.haarcascades + 'haarcascade_frontalface_default.xml'\n",
        "\n",
        "# Remove the check and initialization\n",
        "# if not os.path.exists(haar_cascade_path):\n",
        "#     print(f\"Error: Haar Cascade file not found at {haar_cascade_path}\")\n",
        "#     print(\"You might need to find its location or download it manually.\")\n",
        "#     face_cascade = None # Set cascade to None if file is not found\n",
        "# else:\n",
        "#     # Initialize the Haar Cascade face detector\n",
        "#     face_cascade = cv2.CascadeClassifier(haar_cascade_path)\n",
        "\n",
        "# if face_cascade is not None and face_cascade.empty():\n",
        "#     print(\"Error: Failed to load Haar Cascade classifier.\")\n",
        "#     face_cascade = None\n",
        "# else:\n",
        "#     print(\"OpenCV Haar Cascade face detector initialized successfully.\")\n",
        "\n",
        "print(\"OpenCV Haar Cascade initialization skipped as face detection is being replaced by simple cropping.\")\n",
        "face_cascade = None # Explicitly set to None as it's not used"
      ],
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "OpenCV Haar Cascade initialization skipped as face detection is being replaced by simple cropping.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "b3ce0c48"
      },
      "source": [
        "### **2.3. Prepare Processed Directory Structure (Siapkan Struktur Direktori Hasil Proses)**\n",
        "\n",
        "**Penjelasan:** Membuat direktori tujuan untuk data yang sudah diproses (`PROCESSED_PATH`) dan sub-direktori `train` dan `test` di dalamnya. Kemudian, mengidentifikasi kelas-kelas (mahasiswa) dari nama file mentah dan membuat sub-folder untuk setiap kelas di dalam direktori `train` dan `test` yang sudah diproses."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "40f7514e",
        "outputId": "b833780c-6cbb-43a7-f03e-2276c35ce1f1"
      },
      "source": [
        "# Remove existing processed directory if it exists to start fresh\n",
        "if os.path.exists(PROCESSED_PATH):\n",
        "    print(f\"Removing existing processed directory: {PROCESSED_PATH}\")\n",
        "    shutil.rmtree(PROCESSED_PATH)\n",
        "\n",
        "# Create the main processed directory\n",
        "os.makedirs(PROCESSED_PATH, exist_ok=True)\n",
        "\n",
        "# Define train and test paths within the processed directory\n",
        "PROCESSED_TRAIN_DIR = os.path.join(PROCESSED_PATH, 'train')\n",
        "PROCESSED_TEST_DIR = os.path.join(PROCESSED_PATH, 'test')\n",
        "\n",
        "# Create train and test sub-directories\n",
        "os.makedirs(PROCESSED_TRAIN_DIR, exist_ok=True)\n",
        "os.makedirs(PROCESSED_TEST_DIR, exist_ok=True)\n",
        "\n",
        "print(f\"Created processed directories: {PROCESSED_TRAIN_DIR} and {PROCESSED_TEST_DIR}\")\n",
        "\n",
        "# Identify class names (NIMs) from the filenames in the raw train and test directories\n",
        "# We need to look inside 'Data Train' and 'Data Test' to get all NIMs\n",
        "raw_train_folder = os.path.join(RAW_DATA_PATH, 'Data Train')\n",
        "raw_test_folder = os.path.join(RAW_DATA_PATH, 'Data Test')\n",
        "\n",
        "all_raw_image_files = []\n",
        "if os.path.exists(raw_train_folder):\n",
        "    all_raw_image_files.extend(glob.glob(os.path.join(raw_train_folder, '*.jpg')))\n",
        "if os.path.exists(raw_test_folder):\n",
        "    all_raw_image_files.extend(glob.glob(os.path.join(raw_test_folder, '*.jpg')))\n",
        "\n",
        "\n",
        "# Extract unique NIMs (first 10 characters of filename)\n",
        "class_nims = sorted(list(set([os.path.basename(f)[:10] for f in all_raw_image_files if len(os.path.basename(f)) >= 10])))\n",
        "\n",
        "\n",
        "if not class_nims:\n",
        "    print(\"Error: No class NIMs found based on filename format (first 10 characters) in Data Train or Data Test folders. Please check filenames.\")\n",
        "else:\n",
        "    print(f\"\\nIdentified {len(class_nims)} unique classes (NIMs). Example: {class_nims[:10]}...\")\n",
        "\n",
        "    # Create class sub-folders in both train and test processed directories\n",
        "    for nim in class_nims:\n",
        "        os.makedirs(os.path.join(PROCESSED_TRAIN_DIR, nim), exist_ok=True)\n",
        "        os.makedirs(os.path.join(PROCESSED_TEST_DIR, nim), exist_ok=True)\n",
        "    print(f\"\\nCreated sub-folders for {len(class_nims)} classes in {PROCESSED_TRAIN_DIR} and {PROCESSED_TEST_DIR}.\")\n",
        "\n",
        "# Update NUM_CLASSES global variable\n",
        "NUM_CLASSES = len(class_nims)\n",
        "print(f\"Updated NUM_CLASSES: {NUM_CLASSES}\")"
      ],
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Removing existing processed directory: /content/processed_dataset\n",
            "Created processed directories: /content/processed_dataset/train and /content/processed_dataset/test\n",
            "\n",
            "Identified 53 unique classes (NIMs). Example: ['5221911012', '5221911025', '5231811002', '5231811004', '5231811005', '5231811006', '5231811007', '5231811008', '5231811009', '5231811010']...\n",
            "\n",
            "Created sub-folders for 53 classes in /content/processed_dataset/train and /content/processed_dataset/test.\n",
            "Updated NUM_CLASSES: 53\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9c779919"
      },
      "source": [
        "### **2.4. Run the Simple Cropping Pipeline (Jalankan Pipeline Pemotongan Sederhana)**\n",
        "\n",
        "**Penjelasan:** Fungsi ini akan membaca gambar dari direktori sumber, melakukan pemotongan di bagian tengah gambar (simple center crop), mengubah ukurannya, dan menyimpannya ke direktori tujuan yang sudah distrukturkan per kelas. Kita akan membagi data secara manual ke direktori train/test selama proses penyimpanan. **Catatan:** Langkah ini menggantikan deteksi wajah yang lebih kompleks demi kecepatan, dengan asumsi wajah berada di tengah gambar."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bb27a5f4",
        "outputId": "108463b4-a756-4f3c-9570-e79c7d9a1b39"
      },
      "source": [
        "def process_and_save_faces_by_split(raw_train_dir, raw_test_dir, processed_train_dir, processed_test_dir, img_width, img_height):\n",
        "    \"\"\"\n",
        "    Processes images directly from raw train and test directories by performing a simple\n",
        "    center crop and resize, extracting class name (NIM) from the filename,\n",
        "    and saves them to the corresponding processed train and test directories\n",
        "    under the correct class subfolder. Assumes filenames are in the format NIM_...jpg.\n",
        "\n",
        "    Args:\n",
        "        raw_train_dir (str): Directory containing raw training images (all in one folder per split).\n",
        "        raw_test_dir (str): Directory containing raw testing images (all in one folder per split).\n",
        "        processed_train_dir (str): Directory to save processed training images (with class subdirectories).\n",
        "        processed_test_dir (str): Directory to save processed testing images (with class subdirectories).\n",
        "        img_width (int): Target width for cropped face images.\n",
        "        img_height (int): Target height for cropped face images.\n",
        "    \"\"\"\n",
        "\n",
        "    total_processed_train = 0\n",
        "    skipped_error_train = 0\n",
        "\n",
        "    print(f\"\\nStarting simple center cropping for training data from {raw_train_dir}...\")\n",
        "\n",
        "    # Process training data - iterate directly through image files\n",
        "    image_files_train = glob.glob(os.path.join(raw_train_dir, '*.jpg'))\n",
        "    # print(f\"Found {len(image_files_train)} images in {raw_train_dir} for training\") # Debug print\n",
        "\n",
        "    for img_path in image_files_train:\n",
        "        try:\n",
        "            # Read image\n",
        "            img = cv2.imread(img_path)\n",
        "            if img is None:\n",
        "                print(f\"Warning: Could not read image {img_path}. Skipping.\")\n",
        "                skipped_error_train += 1\n",
        "                continue\n",
        "\n",
        "            # Get image dimensions\n",
        "            h, w, _ = img.shape\n",
        "\n",
        "            # Determine crop dimensions (simple center crop)\n",
        "            min_dim = min(h, w)\n",
        "            start_x = max(0, int((w - min_dim) / 2))\n",
        "            start_y = max(0, int((h - min_dim) / 2))\n",
        "            end_x = start_x + min_dim\n",
        "            end_y = start_y + min_dim\n",
        "\n",
        "            # Crop the center square\n",
        "            center_crop = img[start_y:end_y, start_x:end_x]\n",
        "\n",
        "            # Resize the cropped image to the target size\n",
        "            img_resized = cv2.resize(center_crop, (img_width, img_height))\n",
        "\n",
        "            # Extract NIM (class name) from filename\n",
        "            filename = os.path.basename(img_path)\n",
        "            if len(filename) < 10:\n",
        "                 print(f\"Warning: Filename {filename} is too short to extract NIM. Skipping.\")\n",
        "                 skipped_error_train += 1\n",
        "                 continue\n",
        "\n",
        "            nim = filename[:10] # Assuming first 10 chars are NIM\n",
        "\n",
        "            # Define save directory and path under the correct NIM subfolder\n",
        "            save_dir = os.path.join(processed_train_dir, nim)\n",
        "            os.makedirs(save_dir, exist_ok=True) # Ensure the class directory exists\n",
        "            save_path = os.path.join(save_dir, filename)\n",
        "\n",
        "            # Save the processed image\n",
        "            cv2.imwrite(save_path, img_resized)\n",
        "            total_processed_train += 1\n",
        "\n",
        "        except Exception as e:\n",
        "            print(f\"An error occurred while processing {img_path}: {e}\")\n",
        "            skipped_error_train += 1\n",
        "            continue\n",
        "\n",
        "    print(f\"\\nSimple center cropping for training data complete.\")\n",
        "    print(f\"Total training images processed: {total_processed_train}\")\n",
        "    print(f\"Training images skipped (error): {skipped_error_train}\")\n",
        "\n",
        "    total_processed_test = 0\n",
        "    skipped_error_test = 0\n",
        "\n",
        "    print(f\"\\nStarting simple center cropping for testing data from {raw_test_dir}...\")\n",
        "\n",
        "    # Process testing data - iterate directly through image files\n",
        "    image_files_test = glob.glob(os.path.join(raw_test_dir, '*.jpg'))\n",
        "    # print(f\"Found {len(image_files_test)} images in {raw_test_dir} for testing\") # Debug print\n",
        "\n",
        "    for img_path in image_files_test:\n",
        "        try:\n",
        "            # Read image\n",
        "            img = cv2.imread(img_path)\n",
        "            if img is None:\n",
        "                print(f\"Warning: Could not read image {img_path}. Skipping.\")\n",
        "                skipped_error_test += 1\n",
        "                continue\n",
        "\n",
        "            # Get image dimensions\n",
        "            h, w, _ = img.shape\n",
        "\n",
        "            # Determine crop dimensions (simple center crop)\n",
        "            min_dim = min(h, w)\n",
        "            start_x = max(0, int((w - min_dim) / 2))\n",
        "            start_y = max(0, int((h - min_dim) / 2))\n",
        "            end_x = start_x + min_dim\n",
        "            end_y = start_y + min_dim\n",
        "\n",
        "            # Crop the center square\n",
        "            center_crop = img[start_y:end_y, start_x:end_x]\n",
        "\n",
        "            # Resize the cropped image to the target size\n",
        "            img_resized = cv2.resize(center_crop, (img_width, img_height))\n",
        "\n",
        "            # Extract NIM (class name) from filename\n",
        "            filename = os.path.basename(img_path)\n",
        "            if len(filename) < 10:\n",
        "                 print(f\"Warning: Filename {filename} is too short to extract NIM. Skipping.\")\n",
        "                 skipped_error_test += 1\n",
        "                 continue\n",
        "\n",
        "            nim = filename[:10] # Assuming first 10 chars are NIM\n",
        "\n",
        "\n",
        "            # Define save directory and path under the correct NIM subfolder\n",
        "            save_dir = os.path.join(processed_test_dir, nim)\n",
        "            os.makedirs(save_dir, exist_ok=True) # Ensure the class directory exists\n",
        "            save_path = os.path.join(save_dir, filename)\n",
        "\n",
        "            # Save the processed image\n",
        "            cv2.imwrite(save_path, img_resized)\n",
        "            total_processed_test += 1\n",
        "\n",
        "        except Exception as e:\n",
        "            print(f\"An error occurred while processing {img_path}: {e}\")\n",
        "            skipped_error_test += 1\n",
        "            continue\n",
        "\n",
        "    print(f\"\\nSimple center cropping for testing data complete.\")\n",
        "    print(f\"Total testing images processed: {total_processed_test}\")\n",
        "    print(f\"Testing images skipped (error): {skipped_error_test}\")\n",
        "\n",
        "\n",
        "# Run the pipeline\n",
        "# Assumes RAW_DATA_PATH now points to the directory containing 'Data Train' and 'Data Test' subfolders\n",
        "raw_train_folder = os.path.join(RAW_DATA_PATH, 'Data Train')\n",
        "raw_test_folder = os.path.join(RAW_DATA_PATH, 'Data Test')\n",
        "\n",
        "\n",
        "# Check if the expected raw subfolders exist\n",
        "if not os.path.exists(raw_train_folder):\n",
        "    print(f\"Error: Raw training data folder not found at {raw_train_folder}. Please check your extracted data structure.\")\n",
        "elif not os.path.exists(raw_test_folder):\n",
        "     print(f\"Error: Raw testing data folder not found at {raw_test_folder}. Please check your extracted data structure.\")\n",
        "else:\n",
        "    process_and_save_faces_by_split(\n",
        "        raw_train_dir=raw_train_folder,\n",
        "        raw_test_dir=raw_test_folder,\n",
        "        processed_train_dir=PROCESSED_TRAIN_DIR,\n",
        "        processed_test_dir=PROCESSED_TEST_DIR,\n",
        "        img_width=IMG_WIDTH,\n",
        "        img_height=IMG_HEIGHT\n",
        "    )"
      ],
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Starting simple center cropping for training data from /content/raw_dataset/Dataset Sistem Presensi Wajah V1.0/Data Train...\n",
            "\n",
            "Simple center cropping for training data complete.\n",
            "Total training images processed: 1665\n",
            "Training images skipped (error): 0\n",
            "\n",
            "Starting simple center cropping for testing data from /content/raw_dataset/Dataset Sistem Presensi Wajah V1.0/Data Test...\n",
            "\n",
            "Simple center cropping for testing data complete.\n",
            "Total testing images processed: 416\n",
            "Testing images skipped (error): 0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "813d24ed"
      },
      "source": [
        "### **2.5. Verify Processed Dataset (Verifikasi Dataset Hasil Proses)**\n",
        "\n",
        "**Penjelasan:** Menghitung dan mencetak jumlah gambar di direktori training dan testing yang sudah diproses untuk memastikan bahwa pipeline deteksi dan pemotongan wajah berhasil."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "197aac03",
        "outputId": "0f2e55eb-7664-43f4-920e-6d0b3d9224e2"
      },
      "source": [
        "# Count images in processed directories\n",
        "processed_train_count = sum([len(files) for r, d, files in os.walk(PROCESSED_TRAIN_DIR)])\n",
        "processed_test_count = sum([len(files) for r, d, files in os.walk(PROCESSED_TEST_DIR)])\n",
        "\n",
        "print(f\"Jumlah total gambar di Data Train (setelah proses): {processed_train_count}\")\n",
        "print(f\"Jumlah total gambar di Data Test (setelah proses): {processed_test_count}\")\n",
        "\n",
        "# Verify class counts in processed directories\n",
        "processed_train_class_counts = {}\n",
        "for class_folder in os.listdir(PROCESSED_TRAIN_DIR):\n",
        "    class_path = os.path.join(PROCESSED_TRAIN_DIR, class_folder)\n",
        "    if os.path.isdir(class_path):\n",
        "        processed_train_class_counts[class_folder] = len(os.listdir(class_path))\n",
        "\n",
        "print(f\"\\nJumlah kelas (mahasiswa) di Data Train (setelah proses): {len(processed_train_class_counts)}\")\n",
        "# print(\"Jumlah gambar per kelas di Data Train (setelah proses):\")\n",
        "# for cls, count in processed_train_class_counts.items():\n",
        "#     print(f\"  {cls}: {count}\")\n",
        "\n",
        "processed_test_class_counts = {}\n",
        "for class_folder in os.listdir(PROCESSED_TEST_DIR):\n",
        "    class_path = os.path.join(PROCESSED_TEST_DIR, class_folder)\n",
        "    if os.path.isdir(class_path):\n",
        "        processed_test_class_counts[class_folder] = len(os.listdir(class_path))\n",
        "\n",
        "print(f\"Jumlah kelas (mahasiswa) di Data Test (setelah proses): {len(processed_test_class_counts)}\")\n",
        "# print(\"Jumlah gambar per kelas di Data Test (setelah proses):\")\n",
        "# for cls, count in processed_test_class_counts.items():\n",
        "#     print(f\"  {cls}: {count}\")\n",
        "\n",
        "# Check if the number of classes matches the expected NUM_CLASSES\n",
        "if len(processed_train_class_counts) == NUM_CLASSES and len(processed_test_class_counts) == NUM_CLASSES:\n",
        "    print(\"\\nJumlah kelas di direktori proses sesuai dengan jumlah kelas yang terdeteksi.\")\n",
        "elif len(processed_train_class_counts) != NUM_CLASSES:\n",
        "    print(f\"\\nWarning: Jumlah kelas di Data Train proses ({len(processed_train_class_counts)}) tidak sesuai dengan jumlah kelas yang terdeteksi ({NUM_CLASSES}).\")\n",
        "elif len(processed_test_class_counts) != NUM_CLASSES:\n",
        "     print(f\"\\nWarning: Jumlah kelas di Data Test proses ({len(processed_test_class_counts)}) tidak sesuai dengan jumlah kelas yang terdeteksi ({NUM_CLASSES}).\")\n",
        "else:\n",
        "     print(\"\\nWarning: Jumlah kelas di direktori proses tidak sesuai dengan jumlah kelas yang terdeteksi.\")"
      ],
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Jumlah total gambar di Data Train (setelah proses): 1665\n",
            "Jumlah total gambar di Data Test (setelah proses): 416\n",
            "\n",
            "Jumlah kelas (mahasiswa) di Data Train (setelah proses): 53\n",
            "Jumlah kelas (mahasiswa) di Data Test (setelah proses): 53\n",
            "\n",
            "Jumlah kelas di direktori proses sesuai dengan jumlah kelas yang terdeteksi.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "df242579"
      },
      "source": [
        "### 2.6. Inspect Class Distribution (Inspeksi Distribusi Kelas)\n",
        "\n",
        "**Penjelasan:** Memvisualisasikan jumlah gambar per kelas di direktori training yang sudah diproses untuk melihat apakah pembagian train/test atau proses deteksi wajah menyebabkan ketidakseimbangan yang signifikan."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6730b013"
      },
      "source": [
        "# Sort classes by count for better visualization\n",
        "if processed_train_class_counts:\n",
        "    sorted_processed_class_counts = dict(sorted(processed_train_class_counts.items(), key=lambda item: item[1]))\n",
        "\n",
        "    # Plot class distribution for processed data\n",
        "    plt.figure(figsize=(15, 7))\n",
        "    plt.bar(sorted_processed_class_counts.keys(), sorted_processed_class_counts.values())\n",
        "    plt.xticks(rotation=90)\n",
        "    plt.xlabel(\"Nama Mahasiswa (Kelas - NIM)\") # Use NIM as class name\n",
        "    plt.ylabel(\"Jumlah Gambar di Data Train (Setelah Proses)\")\n",
        "    plt.title(\"Distribusi Jumlah Gambar per Mahasiswa (Data Train Setelah Proses)\")\n",
        "    plt.tight_layout()\n",
        "    plt.show()\n",
        "\n",
        "    # Check if all class counts are the same after processing\n",
        "    if len(processed_train_class_counts) > 0:\n",
        "        first_count_processed = list(processed_train_class_counts.values())[0]\n",
        "        all_counts_same_processed = all(count == first_count_processed for count in processed_train_class_counts.values())\n",
        "\n",
        "        if all_counts_same_processed:\n",
        "            print(\"\\nSemua mahasiswa memiliki jumlah gambar yang sama di Data Train (setelah proses).\")\n",
        "        else:\n",
        "            print(\"\\nJumlah gambar per mahasiswa di Data Train (setelah proses) bervariasi.\")\n",
        "    else:\n",
        "        print(\"\\nTidak ada data kelas yang ditemukan di Data Train (setelah proses) untuk diperiksa distribusinya.\")\n",
        "\n",
        "else:\n",
        "    print(\"\\nTidak ada data kelas yang ditemukan di Data Train (setelah proses) untuk diperiksa distribusinya.\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "79716938"
      },
      "source": [
        "### 2.7. Visualize Sample Images (Visualisasi Sampel Gambar)\n",
        "\n",
        "**Penjelasan:** Menampilkan beberapa gambar acak dari direktori training yang sudah diproses beserta label (NIM) mereka untuk melihat hasil dari proses deteksi dan pemotongan wajah."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fc85f24c"
      },
      "source": [
        "plt.figure(figsize=(15, 12)) # Adjust figure size for more images\n",
        "# Get all image files from the processed training directory\n",
        "all_processed_train_images = glob.glob(os.path.join(PROCESSED_TRAIN_DIR, '*', '*.jpg'))\n",
        "\n",
        "# Check if there are enough images\n",
        "num_samples_to_display = min(20, len(all_processed_train_images)) # Display up to 20 samples\n",
        "if num_samples_to_display == 0:\n",
        "    print(\"Tidak ada gambar di direktori training yang sudah diproses untuk ditampilkan.\")\n",
        "else:\n",
        "    # Select random images\n",
        "    sample_images_paths_processed = np.random.choice(all_processed_train_images, size=num_samples_to_display, replace=False)\n",
        "\n",
        "    for i, img_path in enumerate(sample_images_paths_processed):\n",
        "        plt.subplot(4, 5, i + 1) # Adjust subplot grid to 4 rows and 5 columns\n",
        "        img = plt.imread(img_path) # imread in matplotlib reads as RGB\n",
        "        plt.imshow(img)\n",
        "\n",
        "        # Extract class name (NIM) from directory name\n",
        "        class_name_processed = os.path.basename(os.path.dirname(img_path))\n",
        "\n",
        "        plt.title(class_name_processed)\n",
        "        plt.axis(\"off\")\n",
        "\n",
        "    plt.tight_layout()\n",
        "    plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}