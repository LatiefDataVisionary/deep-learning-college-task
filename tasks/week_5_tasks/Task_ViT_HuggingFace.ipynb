{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "toc_visible": true,
      "authorship_tag": "ABX9TyN9KFy9xG6BX+pQ61xXGyaH",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/LatiefDataVisionary/deep-learning-college-task/blob/main/tasks/week_5_tasks/Task_ViT_HuggingFace.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2a2672bf"
      },
      "source": [
        "## **Section 0: Initial Setup (Pengaturan Awal)**\n",
        "\n",
        "**Penjelasan:** Bagian ini untuk melakukan instalasi library penting yang mungkin belum ada di Colab dan menghubungkan Google Drive."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "f7887a22"
      },
      "source": [
        "### **0.1. Install Libraries (Instalasi Library)**\n",
        "\n",
        "**Penjelasan:** Menginstal library tambahan yang mungkin diperlukan, yaitu `mtcnn` yang merupakan library kunci untuk deteksi wajah."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "a981b556",
        "outputId": "420e494a-e4b6-4350-8b11-6595b5e29fc4"
      },
      "source": [
        "# Install mtcnn library\n",
        "!pip install opencv"
      ],
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[31mERROR: Could not find a version that satisfies the requirement opencv (from versions: none)\u001b[0m\u001b[31m\n",
            "\u001b[0m\u001b[31mERROR: No matching distribution found for opencv\u001b[0m\u001b[31m\n",
            "\u001b[0m"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0cd8f58a"
      },
      "source": [
        "### **0.2. Mount Google Drive (Menghubungkan Google Drive**)\n",
        "\n",
        "**Penjelasan:** Menghubungkan notebook ini dengan akun Google Drive Anda. Ini diperlukan agar notebook dapat membaca file dataset gambar yang telah Anda simpan di Google Drive. Setelah menjalankan sel ini, ikuti instruksi otorisasi yang muncul."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7c510c84",
        "outputId": "ae08a8df-0b05-4de0-f7d5-3a8c96e552c8"
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "647d31f8"
      },
      "source": [
        "## **Section 1: Import Libraries and Environment Setup (Impor Library dan Pengaturan Lingkungan)**\n",
        "\n",
        "**Penjelasan:** Mengimpor semua modul yang dibutuhkan dan mendefinisikan variabel-variabel global, termasuk path untuk data mentah dan data yang akan diproses."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "29c5d6a5"
      },
      "source": [
        "### 1.1. Import Core Libraries (Impor Library Utama)\n",
        "\n",
        "**Penjelasan:** Mengimpor library utama seperti tensorflow, keras, numpy, matplotlib, os, zipfile, cv2 (OpenCV), glob, shutil yang akan digunakan sepanjang proyek ini."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2f552c82"
      },
      "source": [
        "import tensorflow as tf\n",
        "from tensorflow import keras\n",
        "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Conv2D, MaxPooling2D, Flatten, Dense, Dropout, BatchNormalization, InputLayer\n",
        "from tensorflow.keras.optimizers import Adam\n",
        "from tensorflow.keras.callbacks import ModelCheckpoint, EarlyStopping\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import os\n",
        "import zipfile\n",
        "import cv2 # Import OpenCV\n",
        "import glob # To list files\n",
        "import shutil # To manage directories\n",
        "from sklearn.metrics import classification_report, confusion_matrix\n",
        "import seaborn as sns # For confusion matrix visualization"
      ],
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1e572564"
      },
      "source": [
        "### **1.2. Define Configurations (Definisi Konfigurasi)**\n",
        "\n",
        "**Penjelasan:** Mendefinisikan variabel-variabel konfigurasi yang akan digunakan di seluruh notebook, termasuk path ke dataset, ukuran gambar yang akan digunakan, ukuran batch untuk training, jumlah epoch, dan jumlah kelas (mahasiswa)."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "48aff23b"
      },
      "source": [
        "# Define Directory Paths (Definisi Path Direktori)\n",
        "# Path ke file zip di Google Drive\n",
        "ZIP_PATH = '/content/drive/MyDrive/Dataset/Dataset Sistem Presensi Wajah V1.0.zip'\n",
        "# Path untuk mengekstrak data mentah (sebelum deteksi wajah)\n",
        "RAW_DATA_PATH = '/content/raw_dataset'\n",
        "# Path untuk menyimpan dataset yang sudah bersih (setelah deteksi wajah)\n",
        "PROCESSED_PATH = '/content/processed_dataset'\n",
        "\n",
        "# Define Image Parameters (Definisi Parameter Gambar)\n",
        "IMG_HEIGHT = 128 # Ukuran yang lebih kecil cocok untuk model dari dasar\n",
        "IMG_WIDTH = 128\n",
        "CHANNELS = 3 # RGB color images\n",
        "\n",
        "# Define Training Parameters (Definisi Parameter Pelatihan)\n",
        "BATCH_SIZE = 32\n",
        "EPOCHS = 50 # Will be controlled by Early Stopping\n",
        "# NUM_CLASSES will be determined by the data generator later\n",
        "NUM_CLASSES = None # Placeholder"
      ],
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "1mmwlN92LofO"
      },
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8f5bd934"
      },
      "source": [
        "## **Section 2: Advanced Preprocessing - Face Detection and Cropping (Preprocessing Lanjutan - Deteksi dan Pemotongan Wajah)**\n",
        "\n",
        "**Penjelasan:** Ini adalah tahap paling krusial dan merupakan upgrade utama. Kita akan memproses seluruh dataset mentah sekali jalan. Tujuannya adalah mendeteksi wajah di setiap gambar, memotongnya, dan menyimpannya ke struktur direktori baru yang bersih dan siap pakai. Ini menyelesaikan masalah distorsi aspect ratio dan noise latar belakang menggunakan deteksi wajah berbasis OpenCV Haar Cascades."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "47fc9757"
      },
      "source": [
        "### **2.1. Unzip Raw Dataset (Ekstrak Dataset Mentah)**\n",
        "\n",
        "**Penjelasan:** Kode untuk mengekstrak file .zip ke RAW_DATA_PATH."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "454e6dae",
        "outputId": "8127744c-f8af-42a8-f099-f94040fc8376"
      },
      "source": [
        "print(f\"Extracting {ZIP_PATH} to {RAW_DATA_PATH}...\")\n",
        "try:\n",
        "    with zipfile.ZipFile(ZIP_PATH, 'r') as zip_ref:\n",
        "        zip_ref.extractall(RAW_DATA_PATH)\n",
        "    print(\"Extraction complete.\")\n",
        "except FileNotFoundError:\n",
        "    print(f\"Error: Zip file not found at {ZIP_PATH}\")\n",
        "except zipfile.BadZipFile:\n",
        "    print(f\"Error: Could not open or read zip file at {ZIP_PATH}. It might be corrupted.\")\n",
        "except Exception as e:\n",
        "    print(f\"An error occurred during extraction: {e}\")\n",
        "\n",
        "# Verify extraction\n",
        "if os.path.exists(RAW_DATA_PATH):\n",
        "    print(f\"\\nContents of {RAW_DATA_PATH} after extraction: {os.listdir(RAW_DATA_PATH)}\")\n",
        "else:\n",
        "    print(f\"\\nError: Raw data directory not found after extraction at {RAW_DATA_PATH}\")\n",
        "\n",
        "# Assuming the extracted content is in a subfolder within RAW_DATA_PATH\n",
        "# Let's find the actual folder containing the images\n",
        "raw_image_folder = None\n",
        "for item in os.listdir(RAW_DATA_PATH):\n",
        "    item_path = os.path.join(RAW_DATA_PATH, item)\n",
        "    if os.path.isdir(item_path):\n",
        "        # Simple check: if it contains subfolders or files ending with .jpg\n",
        "        if any(os.path.isdir(os.path.join(item_path, sub_item)) for sub_item in os.listdir(item_path)) or \\\n",
        "           any(sub_item.endswith('.jpg') for sub_item in os.listdir(item_path)):\n",
        "           raw_image_folder = item_path\n",
        "           break\n",
        "\n",
        "if raw_image_folder:\n",
        "    print(f\"\\nIdentified raw image folder: {raw_image_folder}\")\n",
        "    # Update RAW_DATA_PATH to point directly to the folder containing the images\n",
        "    RAW_DATA_PATH = raw_image_folder\n",
        "    print(f\"Updated RAW_DATA_PATH: {RAW_DATA_PATH}\")\n",
        "else:\n",
        "    print(\"\\nWarning: Could not identify the main folder containing raw images within the extracted directory.\")\n",
        "    print(\"Please manually inspect the extracted contents and update RAW_DATA_PATH if necessary.\")"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Extracting /content/drive/MyDrive/Dataset/Dataset Sistem Presensi Wajah V1.0.zip to /content/raw_dataset...\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "e827b231"
      },
      "source": [
        "### **2.2. Initialize Face Detector (OpenCV Haar Cascade)**\n",
        "\n",
        "**Penjelasan:** Memuat file Haar Cascade pre-trained untuk deteksi wajah dari OpenCV. Kita akan menggunakan detektor wajah frontal default."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "30b5147d"
      },
      "source": [
        "# Download the Haar Cascade XML file\n",
        "# This is a standard location in Colab, but you might need to adjust if it's not found\n",
        "haar_cascade_path = cv2.data.haarcascades + 'haarcascade_frontalface_default.xml'\n",
        "\n",
        "# Check if the file exists, if not, provide a message\n",
        "if not os.path.exists(haar_cascade_path):\n",
        "    print(f\"Error: Haar Cascade file not found at {haar_cascade_path}\")\n",
        "    print(\"You might need to find its location or download it manually.\")\n",
        "    face_cascade = None # Set cascade to None if file is not found\n",
        "else:\n",
        "    # Initialize the Haar Cascade face detector\n",
        "    face_cascade = cv2.CascadeClassifier(haar_cascade_path)\n",
        "\n",
        "if face_cascade is not None and face_cascade.empty():\n",
        "    print(\"Error: Failed to load Haar Cascade classifier.\")\n",
        "    face_cascade = None\n",
        "else:\n",
        "    print(\"OpenCV Haar Cascade face detector initialized successfully.\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "b3ce0c48"
      },
      "source": [
        "### **2.3. Prepare Processed Directory Structure (Siapkan Struktur Direktori Hasil Proses)**\n",
        "\n",
        "**Penjelasan:** Membuat direktori tujuan untuk data yang sudah diproses (`PROCESSED_PATH`) dan sub-direktori `train` dan `test` di dalamnya. Kemudian, mengidentifikasi kelas-kelas (mahasiswa) dari nama file mentah dan membuat sub-folder untuk setiap kelas di dalam direktori `train` dan `test` yang sudah diproses."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "40f7514e"
      },
      "source": [
        "# Remove existing processed directory if it exists to start fresh\n",
        "if os.path.exists(PROCESSED_PATH):\n",
        "    print(f\"Removing existing processed directory: {PROCESSED_PATH}\")\n",
        "    shutil.rmtree(PROCESSED_PATH)\n",
        "\n",
        "# Create the main processed directory\n",
        "os.makedirs(PROCESSED_PATH, exist_ok=True)\n",
        "\n",
        "# Define train and test paths within the processed directory\n",
        "PROCESSED_TRAIN_DIR = os.path.join(PROCESSED_PATH, 'train')\n",
        "PROCESSED_TEST_DIR = os.path.join(PROCESSED_PATH, 'test')\n",
        "\n",
        "# Create train and test sub-directories\n",
        "os.makedirs(PROCESSED_TRAIN_DIR, exist_ok=True)\n",
        "os.makedirs(PROCESSED_TEST_DIR, exist_ok=True)\n",
        "\n",
        "print(f\"Created processed directories: {PROCESSED_TRAIN_DIR} and {PROCESSED_TEST_DIR}\")\n",
        "\n",
        "# Identify class names (NIMs) from the subdirectories within RAW_DATA_PATH\n",
        "# Assuming RAW_DATA_PATH now points to the main folder containing class subfolders\n",
        "class_nims = sorted([d for d in os.listdir(RAW_DATA_PATH) if os.path.isdir(os.path.join(RAW_DATA_PATH, d))])\n",
        "\n",
        "\n",
        "if not class_nims:\n",
        "    print(\"Warning: No class subfolders found within the raw data directory. Please check the structure of the extracted data.\")\n",
        "else:\n",
        "    print(f\"\\nIdentified {len(class_nims)} unique classes (NIMs) from subfolders. Example: {class_nims[:10]}...\")\n",
        "\n",
        "    # Create class sub-folders in both train and test processed directories\n",
        "    for nim in class_nims:\n",
        "        os.makedirs(os.path.join(PROCESSED_TRAIN_DIR, nim), exist_ok=True)\n",
        "        os.makedirs(os.path.join(PROCESSED_TEST_DIR, nim), exist_ok=True)\n",
        "    print(f\"\\nCreated sub-folders for {len(class_nims)} classes in {PROCESSED_TRAIN_DIR} and {PROCESSED_TEST_DIR}.\")\n",
        "\n",
        "# Update NUM_CLASSES global variable\n",
        "NUM_CLASSES = len(class_nims)\n",
        "print(f\"Updated NUM_CLASSES: {NUM_CLASSES}\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5ec5a1ea"
      },
      "source": [
        "### **2.4. Run the Face Detection & Cropping Pipeline (Jalankan Pipeline Deteksi & Pemotongan Wajah)**\n",
        "\n",
        "**Penjelasan:** Fungsi ini akan membaca gambar dari direktori sumber, mendeteksi wajah menggunakan OpenCV Haar Cascade, memotong wajah yang terdeteksi, dan menyimpannya ke direktori tujuan yang sudah distrukturkan per kelas. Kita akan membagi data secara manual ke direktori train/test selama proses penyimpanan."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bb27a5f4",
        "outputId": "c2e46990-9d1e-4dd1-aa92-fb97826432ab"
      },
      "source": [
        "import random\n",
        "import os\n",
        "import cv2\n",
        "import glob\n",
        "import shutil\n",
        "\n",
        "def process_and_save_faces_opencv(source_dir, processed_train_dir, processed_test_dir, face_cascade, img_width, img_height, test_split=0.2):\n",
        "    \"\"\"\n",
        "    Processes images in source_dir (including subdirectories), detects faces using OpenCV Haar Cascade,\n",
        "    crops them, and saves them to processed_train_dir or processed_test_dir\n",
        "    based on the test_split ratio. Assumes source_dir contains subdirectories named after classes (NIMs).\n",
        "\n",
        "    Args:\n",
        "        source_dir (str): Directory containing class subdirectories.\n",
        "        processed_train_dir (str): Directory to save processed training images.\n",
        "        processed_test_dir (str): Directory to save processed testing images.\n",
        "        face_cascade (cv2.CascadeClassifier): Initialized OpenCV Haar Cascade detector.\n",
        "        img_width (int): Target width for cropped face images.\n",
        "        img_height (int): Target height for cropped face images.\n",
        "        test_split (float): Ratio of data to be allocated to the test set (0.0 to 1.0).\n",
        "    \"\"\"\n",
        "    if face_cascade is None:\n",
        "        print(\"Face cascade not initialized. Skipping face detection and cropping.\")\n",
        "        return\n",
        "\n",
        "    total_processed = 0\n",
        "    skipped_no_face = 0\n",
        "    skipped_error = 0\n",
        "\n",
        "    print(f\"\\nStarting face detection and cropping from {source_dir}...\")\n",
        "\n",
        "    # Walk through subdirectories to find images\n",
        "    for class_dir in os.listdir(source_dir):\n",
        "        class_path = os.path.join(source_dir, class_dir)\n",
        "        if os.path.isdir(class_path):\n",
        "            nim = class_dir # Class name is the directory name (NIM)\n",
        "            image_files = glob.glob(os.path.join(class_path, '*.jpg'))\n",
        "            random.shuffle(image_files) # Shuffle files within each class\n",
        "\n",
        "            for img_path in image_files:\n",
        "                try:\n",
        "                    # Read image\n",
        "                    img = cv2.imread(img_path)\n",
        "                    if img is None:\n",
        "                        print(f\"Warning: Could not read image {img_path}. Skipping.\")\n",
        "                        skipped_error += 1\n",
        "                        continue\n",
        "\n",
        "                    # Convert to grayscale for face detection\n",
        "                    gray = cv2.cvtColor(img, cv2.COLOR_BGR2GRAY)\n",
        "\n",
        "                    # Detect faces\n",
        "                    # parameters: image, scaleFactor, minNeighbors, minSize\n",
        "                    faces = face_cascade.detectMultiScale(gray, scaleFactor=1.1, minNeighbors=5, minSize=(30, 30))\n",
        "\n",
        "                    if len(faces) == 0:\n",
        "                        # print(f\"No face detected in {img_path}. Skipping.\")\n",
        "                        skipped_no_face += 1\n",
        "                        continue\n",
        "\n",
        "                    # Assume the first detected face is the primary one (usually the largest)\n",
        "                    (x, y, w, h) = faces[0]\n",
        "\n",
        "                    # Add a margin around the face bounding box\n",
        "                    margin_ratio = 0.2 # Add 20% margin\n",
        "                    x_m = max(0, int(x - w * margin_ratio))\n",
        "                    y_m = max(0, int(y - h * margin_ratio))\n",
        "                    w_m = min(img.shape[1] - x_m, int(w * (1 + 2 * margin_ratio))) # Ensure margin doesn't go beyond image bounds\n",
        "                    h_m = min(img.shape[0] - y_m, int(h * (1 + 2 * margin_ratio))) # Ensure margin doesn't go beyond image bounds\n",
        "\n",
        "\n",
        "                    # Crop the face with margin\n",
        "                    face_crop = img[y_m:y_m+h_m, x_m:x_m+w_m]\n",
        "\n",
        "                    # Resize the cropped face to the target size\n",
        "                    face_resized = cv2.resize(face_crop, (img_width, img_height))\n",
        "\n",
        "                    # Determine whether to save to train or test directory\n",
        "                    if random.random() < test_split:\n",
        "                        save_dir = os.path.join(processed_test_dir, nim)\n",
        "                    else:\n",
        "                        save_dir = os.path.join(processed_train_dir, nim)\n",
        "\n",
        "                    # Ensure the class directory exists (should be already created in 2.3, but good practice)\n",
        "                    os.makedirs(save_dir, exist_ok=True)\n",
        "\n",
        "                    # Define save path using the original filename\n",
        "                    save_path = os.path.join(save_dir, os.path.basename(img_path))\n",
        "\n",
        "                    # Save the processed image\n",
        "                    cv2.imwrite(save_path, face_resized)\n",
        "                    total_processed += 1\n",
        "\n",
        "                except Exception as e:\n",
        "                    print(f\"An error occurred while processing {img_path}: {e}\")\n",
        "                    skipped_error += 1\n",
        "                    continue\n",
        "\n",
        "    print(f\"\\nFace detection and cropping complete.\")\n",
        "    print(f\"Total images processed: {total_processed}\")\n",
        "    print(f\"Images skipped (no face detected): {skipped_no_face}\")\n",
        "    print(f\"Images skipped (error): {skipped_error}\")\n",
        "\n",
        "\n",
        "# Run the pipeline\n",
        "process_and_save_faces_opencv(\n",
        "    source_dir=RAW_DATA_PATH,\n",
        "    processed_train_dir=PROCESSED_TRAIN_DIR,\n",
        "    processed_test_dir=PROCESSED_TEST_DIR,\n",
        "    face_cascade=face_cascade,\n",
        "    img_width=IMG_WIDTH,\n",
        "    img_height=IMG_HEIGHT,\n",
        "    test_split=0.2 # 80% train, 20% test split\n",
        ")"
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Starting face detection and cropping from /content/raw_dataset/Dataset Sistem Presensi Wajah V1.0...\n",
            "\n",
            "Face detection and cropping complete.\n",
            "Total images processed: 0\n",
            "Images skipped (no face detected): 0\n",
            "Images skipped (error): 0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "813d24ed"
      },
      "source": [
        "### 2.5. Verify Processed Dataset (Verifikasi Dataset Hasil Proses)\n",
        "\n",
        "**Penjelasan:** Menghitung dan mencetak jumlah gambar di direktori training dan testing yang sudah diproses untuk memastikan bahwa pipeline deteksi dan pemotongan wajah berhasil."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "197aac03"
      },
      "source": [
        "# Count images in processed directories\n",
        "processed_train_count = sum([len(files) for r, d, files in os.walk(PROCESSED_TRAIN_DIR)])\n",
        "processed_test_count = sum([len(files) for r, d, files in os.walk(PROCESSED_TEST_DIR)])\n",
        "\n",
        "print(f\"Jumlah total gambar di Data Train (setelah proses): {processed_train_count}\")\n",
        "print(f\"Jumlah total gambar di Data Test (setelah proses): {processed_test_count}\")\n",
        "\n",
        "# Verify class counts in processed directories\n",
        "processed_train_class_counts = {}\n",
        "for class_folder in os.listdir(PROCESSED_TRAIN_DIR):\n",
        "    class_path = os.path.join(PROCESSED_TRAIN_DIR, class_folder)\n",
        "    if os.path.isdir(class_path):\n",
        "        processed_train_class_counts[class_folder] = len(os.listdir(class_path))\n",
        "\n",
        "print(f\"\\nJumlah kelas (mahasiswa) di Data Train (setelah proses): {len(processed_train_class_counts)}\")\n",
        "# print(\"Jumlah gambar per kelas di Data Train (setelah proses):\")\n",
        "# for cls, count in processed_train_class_counts.items():\n",
        "#     print(f\"  {cls}: {count}\")\n",
        "\n",
        "processed_test_class_counts = {}\n",
        "for class_folder in os.listdir(PROCESSED_TEST_DIR):\n",
        "    class_path = os.path.join(PROCESSED_TEST_DIR, class_folder)\n",
        "    if os.path.isdir(class_path):\n",
        "        processed_test_class_counts[class_folder] = len(os.listdir(class_path))\n",
        "\n",
        "print(f\"Jumlah kelas (mahasiswa) di Data Test (setelah proses): {len(processed_test_class_counts)}\")\n",
        "# print(\"Jumlah gambar per kelas di Data Test (setelah proses):\")\n",
        "# for cls, count in processed_test_class_counts.items():\n",
        "#     print(f\"  {cls}: {count}\")\n",
        "\n",
        "# Check if the number of classes matches the expected NUM_CLASSES\n",
        "if len(processed_train_class_counts) == NUM_CLASSES and len(processed_test_class_counts) == NUM_CLASSES:\n",
        "    print(\"\\nJumlah kelas di direktori proses sesuai dengan jumlah kelas yang terdeteksi.\")\n",
        "elif len(processed_train_class_counts) != NUM_CLASSES:\n",
        "    print(f\"\\nWarning: Jumlah kelas di Data Train proses ({len(processed_train_class_counts)}) tidak sesuai dengan jumlah kelas yang terdeteksi ({NUM_CLASSES}).\")\n",
        "elif len(processed_test_class_counts) != NUM_CLASSES:\n",
        "     print(f\"\\nWarning: Jumlah kelas di Data Test proses ({len(processed_test_class_counts)}) tidak sesuai dengan jumlah kelas yang terdeteksi ({NUM_CLASSES}).\")\n",
        "else:\n",
        "     print(\"\\nWarning: Jumlah kelas di direktori proses tidak sesuai dengan jumlah kelas yang terdeteksi.\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "df242579"
      },
      "source": [
        "### 2.6. Inspect Class Distribution (Inspeksi Distribusi Kelas)\n",
        "\n",
        "**Penjelasan:** Memvisualisasikan jumlah gambar per kelas di direktori training yang sudah diproses untuk melihat apakah pembagian train/test atau proses deteksi wajah menyebabkan ketidakseimbangan yang signifikan."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6730b013"
      },
      "source": [
        "# Sort classes by count for better visualization\n",
        "if processed_train_class_counts:\n",
        "    sorted_processed_class_counts = dict(sorted(processed_train_class_counts.items(), key=lambda item: item[1]))\n",
        "\n",
        "    # Plot class distribution for processed data\n",
        "    plt.figure(figsize=(15, 7))\n",
        "    plt.bar(sorted_processed_class_counts.keys(), sorted_processed_class_counts.values())\n",
        "    plt.xticks(rotation=90)\n",
        "    plt.xlabel(\"Nama Mahasiswa (Kelas - NIM)\") # Use NIM as class name\n",
        "    plt.ylabel(\"Jumlah Gambar di Data Train (Setelah Proses)\")\n",
        "    plt.title(\"Distribusi Jumlah Gambar per Mahasiswa (Data Train Setelah Proses)\")\n",
        "    plt.tight_layout()\n",
        "    plt.show()\n",
        "\n",
        "    # Check if all class counts are the same after processing\n",
        "    if len(processed_train_class_counts) > 0:\n",
        "        first_count_processed = list(processed_train_class_counts.values())[0]\n",
        "        all_counts_same_processed = all(count == first_count_processed for count in processed_train_class_counts.values())\n",
        "\n",
        "        if all_counts_same_processed:\n",
        "            print(\"\\nSemua mahasiswa memiliki jumlah gambar yang sama di Data Train (setelah proses).\")\n",
        "        else:\n",
        "            print(\"\\nJumlah gambar per mahasiswa di Data Train (setelah proses) bervariasi.\")\n",
        "    else:\n",
        "        print(\"\\nTidak ada data kelas yang ditemukan di Data Train (setelah proses) untuk diperiksa distribusinya.\")\n",
        "\n",
        "else:\n",
        "    print(\"\\nTidak ada data kelas yang ditemukan di Data Train (setelah proses) untuk diperiksa distribusinya.\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "79716938"
      },
      "source": [
        "### 2.7. Visualize Sample Images (Visualisasi Sampel Gambar)\n",
        "\n",
        "**Penjelasan:** Menampilkan beberapa gambar acak dari direktori training yang sudah diproses beserta label (NIM) mereka untuk melihat hasil dari proses deteksi dan pemotongan wajah."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fc85f24c"
      },
      "source": [
        "plt.figure(figsize=(15, 12)) # Adjust figure size for more images\n",
        "# Get all image files from the processed training directory\n",
        "all_processed_train_images = glob.glob(os.path.join(PROCESSED_TRAIN_DIR, '*', '*.jpg'))\n",
        "\n",
        "# Check if there are enough images\n",
        "num_samples_to_display = min(20, len(all_processed_train_images)) # Display up to 20 samples\n",
        "if num_samples_to_display == 0:\n",
        "    print(\"Tidak ada gambar di direktori training yang sudah diproses untuk ditampilkan.\")\n",
        "else:\n",
        "    # Select random images\n",
        "    sample_images_paths_processed = np.random.choice(all_processed_train_images, size=num_samples_to_display, replace=False)\n",
        "\n",
        "    for i, img_path in enumerate(sample_images_paths_processed):\n",
        "        plt.subplot(4, 5, i + 1) # Adjust subplot grid to 4 rows and 5 columns\n",
        "        img = plt.imread(img_path) # imread in matplotlib reads as RGB\n",
        "        plt.imshow(img)\n",
        "\n",
        "        # Extract class name (NIM) from directory name\n",
        "        class_name_processed = os.path.basename(os.path.dirname(img_path))\n",
        "\n",
        "        plt.title(class_name_processed)\n",
        "        plt.axis(\"off\")\n",
        "\n",
        "    plt.tight_layout()\n",
        "    plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}